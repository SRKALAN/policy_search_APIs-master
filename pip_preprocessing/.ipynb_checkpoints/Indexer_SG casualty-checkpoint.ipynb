{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cosmos\n",
      "  Using cached https://files.pythonhosted.org/packages/6a/66/683228000d19273676f8bfc65a3881251cdd405674f692b8fa832b3e1aed/azure_cosmos-4.2.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.6 (from azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting azure-core<2.0.0,>=1.0.0 (from azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/4b/ea7faaafac956a168ab9a95a7ebe583f9d308e8332a68af0ed3128ef520c/azure_core-1.9.0-py2.py3-none-any.whl\n",
      "Collecting requests>=2.18.4 (from azure-core<2.0.0,>=1.0.0->azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests>=2.18.4->azure-core<2.0.0,>=1.0.0->azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests>=2.18.4->azure-core<2.0.0,>=1.0.0->azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2 (from requests>=2.18.4->azure-core<2.0.0,>=1.0.0->azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.18.4->azure-core<2.0.0,>=1.0.0->azure-cosmos)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl\n",
      "Installing collected packages: six, urllib3, idna, chardet, certifi, requests, azure-core, azure-cosmos\n",
      "Successfully installed azure-core-1.9.0 azure-cosmos-4.2.0 certifi-2020.12.5 chardet-4.0.0 idna-2.10 requests-2.25.0 six-1.15.0 urllib3-1.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Using cached https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl\n",
      "Collecting sortedcontainers (from pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/4d/a7046ae1a1a4cc4e9bbed194c387086f06b25038be596543d026946330c9/sortedcontainers-2.3.0-py2.py3-none-any.whl\n",
      "Collecting chardet; python_version > \"3.0\" (from pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl\n",
      "Collecting cryptography (from pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/7c/b6/1f3dd48a22fcd56f19e6cfa95f74ff0a64b046306354e1bd2b936b7c9ab4/cryptography-3.3.1-cp36-abi3-manylinux1_x86_64.whl\n",
      "Collecting six>=1.4.1 (from cryptography->pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting cffi>=1.12 (from cryptography->pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/1a/90fa7e7ee05d91d0339ef264bd8c008f57292aba4a91ec512a0bbb379d1d/cffi-1.14.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pycparser (from cffi>=1.12->cryptography->pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/e7/d9c3a176ca4b02024debf82342dab36efadfc5776f9c8db077e8f6e71821/pycparser-2.20-py2.py3-none-any.whl\n",
      "Installing collected packages: sortedcontainers, chardet, six, pycparser, cffi, cryptography, pdfminer.six\n",
      "Successfully installed cffi-1.14.4 chardet-4.0.0 cryptography-3.3.1 pdfminer.six-20201018 pycparser-2.20 six-1.15.0 sortedcontainers-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app/RB/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import uuid\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords\n",
    "from pdf2txt import convert_pdf\n",
    "# !pip install tika\n",
    "import tika\n",
    "from definitions_v3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsRE_heading=re.compile('(?:\\s*\\([a-z]{1,3}\\)|[A-Z]{1}\\s+[a-zA-Z0-9_\\s]{5})')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "y_cordinate_Re=re.compile('top:(\\d{1,5})px')\n",
    "height_re=re.compile('height:(\\d{1,5})px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_underline(positions,y_cord_text):\n",
    "    positions=[i for i in positions if y_cord_text<=i< y_cord_text+16]\n",
    "    if positions:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_no(soup):\n",
    "    total_items = int(soup.find('span', id='NewReleases_total').text)\n",
    "    items_per_page = int(soup.find('span', id='NewReleases_end').text)\n",
    "    return round(total_items/items_per_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "\n",
    "# print(len(list(extract_pages('test/Chubb16-250-1019 Chubb EBM Business Pack Product Disclosure Statement (PDS) and Policy Wording.pdf'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_underlines(soup):\n",
    "    positions=[]\n",
    "    for span in soup.find_all('span'):\n",
    "        y_cord_list=y_cordinate_Re.findall(str(span))\n",
    "        if y_cord_list:\n",
    "            y_cord=int(y_cord_list[0])\n",
    "        else:\n",
    "            continue\n",
    "        style=\"position:absolute; border: black 1px solid\" in str(span)\n",
    "        height_px_li=height_re.findall(str(span))\n",
    "        if height_px_li:\n",
    "            height_px=height_px_li[0]\n",
    "        else:\n",
    "            continue\n",
    "        height=int(height_px)<15\n",
    "        if all([style,height]):\n",
    "            positions.append(y_cord)\n",
    "    return list(set(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_extract(text_list,language):\n",
    "    ext_flag=False\n",
    "    if language=='english':\n",
    "        ext_lis=['extension']\n",
    "    else:\n",
    "        ext_lis=['extensión','extensiones']\n",
    "   \n",
    "    ext_count=0\n",
    "    position_list=[]\n",
    "    position=0\n",
    "    for text in text_list:\n",
    "        # if any two of 'exclusion ,condition or extension is present in the text, that text is omitted from the logic'\n",
    "        bool_list=['condition' in text.lower(),'exclusion' in text.lower(),'extension' in text.lower()]\n",
    "        if sum(bool_list)>=2:\n",
    "            continue\n",
    "        for i in ext_lis:\n",
    "            for match in re.finditer(i, text.lower()):\n",
    "                position_list.append(position+match.start())               \n",
    "                ext_count+=1\n",
    "        position+=len(text)\n",
    "    if ext_count>=1:\n",
    "        ext_flag=True\n",
    "    return(ext_flag,ext_count,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excl_extract(text_list,language):\n",
    "    exclusion_flag=False\n",
    "    if language=='english':\n",
    "        excl_lis=['exclud','not cover','except','does not mean','not includ','exclusion']\n",
    "    else:\n",
    "        excl_lis=['excepto','excepción','no in cluido','exclusión','excluidos','excluirlo','excluyendo','exclusiones','exclusion','excluyen','excluyentes']\n",
    "   \n",
    "    exclusion_count=0\n",
    "    position_list=[]\n",
    "    position=0\n",
    "    for text in text_list:\n",
    "        # if any two of 'exclusion ,condition or extension is present in the text, that text is omitted from the logic'\n",
    "        bool_list=['condition' in text.lower(),'exclusion' in text.lower(),'extension' in text.lower()]\n",
    "        if sum(bool_list)>=2:\n",
    "            continue\n",
    "        for i in excl_lis:\n",
    "            for match in re.finditer(i, text.lower()):\n",
    "                position_list.append(position+match.start())               \n",
    "                exclusion_count+=1\n",
    "        position+=len(text)\n",
    "    if exclusion_count>=1:\n",
    "        exclusion_flag=True\n",
    "    return(exclusion_flag,exclusion_count,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_extract(text_list,language):\n",
    "    cond_flag=False\n",
    "    if language=='english':\n",
    "        cond_lis=['condition']\n",
    "    else:\n",
    "        cond_lis=['condición','condiciones','condicionado']\n",
    "   \n",
    "    cond_count=0\n",
    "    position_list=[]\n",
    "    position=0\n",
    "    for text in text_list:\n",
    "        # if any two of 'exclusion ,condition or extension is present in the text, that text is omitted from the logic'\n",
    "        bool_list=['condition' in text.lower(),'exclusion' in text.lower(),'extension' in text.lower()]\n",
    "        if sum(bool_list)>=2:\n",
    "            continue\n",
    "        for i in cond_lis:\n",
    "            for match in re.finditer(i, text.lower()):\n",
    "                position_list.append(position+match.start())               \n",
    "                cond_count+=1\n",
    "        position+=len(text)\n",
    "    if cond_count>=1:\n",
    "        cond_flag=True\n",
    "    return(cond_flag,cond_count,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_extraction(soup):\n",
    "    fontsizes=[]\n",
    "    for divs in soup.findAll('div'):\n",
    "        for j in divs.find_all('span'):\n",
    "            ext_size=re.findall(r'font-size:(.*)px\">',str(j))\n",
    "            if ext_size:\n",
    "                fontsizes.append(int(ext_size[0]))\n",
    "    return(fontsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_check(word,text,language):\n",
    "    if language=='spanish':\n",
    "        stop_words = list(set(stopwords.words('spanish')))\n",
    "    else:\n",
    "        stop_words = list(set(stopwords.words('english')))\n",
    "        transition=\"although  instead  whereas  despite  conversely  otherwise  however moreover  likewise  comparatively  correspondingly  similarly  furthermore  additionallyver  rather  nevertheless  nonetheless  regardless  notwithstanding consequently  therefore  thereupon  forthwith  accordingly  henceforth\"\n",
    "        transition_words=transition.split()\n",
    "        transition_words\n",
    "        stop_words.extend(transition_words)\n",
    "\n",
    "    if exclusion_check(text,language):\n",
    "        return True\n",
    "    if word.lower() in stop_words:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclusion_check(text,language):\n",
    "    if language=='spanish':\n",
    "        exclusion_bag=['excepto','excepción','no in cluido','exclusión','excluidos','excluirlo','excluyendo',' excluyen','exclusiones','exclusiones','exclusion','excluyentes']\n",
    "    else:\n",
    "        exclusion_bag=['exclusion','excluded','not covered','will not cover','will not pay']\n",
    "    for word in exclusion_bag:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria_check(text,language):\n",
    "    if language=='spanish':\n",
    "        exclusion_bag=['condición','condiciones','condicionado']\n",
    "    else:\n",
    "        exclusion_bag=['condition','conditions']\n",
    "    for word in exclusion_bag:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_check(text,language):\n",
    "    if language=='english':\n",
    "        ext_bag=['extension']\n",
    "    else:\n",
    "        ext_bag=['extensión','extensiones']\n",
    "    for word in ext_bag:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=re.compile('.*\\d{1,3}.*(?:\\-|\\.|–)')\n",
    "def clean(text):\n",
    "    text=text.replace('\\n','').replace('\\t',' ')\n",
    "    text=start.sub('',text).strip()\n",
    "    \n",
    "\n",
    "    return(text)\n",
    "def clean_pharses(phrases):\n",
    "    phrases=[clean(i) for i in phrases if len(i)> 3]\n",
    "    phrases=list(set(phrases))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_index(path,subdir,language,country):\n",
    "    pointsRE_heading=re.compile('(?:\\s*\\([a-z]{1,3}\\)|(?<![A-z])[A-Z]{1}\\s+[a-zA-Z0-9_\\s]{5})')\n",
    "    pageNumRE=re.compile('Page\\s*(\\d{1,3})',re.IGNORECASE)\n",
    "    neglect_def=['policy','insured','schedule']\n",
    "    local_indexed={}\n",
    "    previous_span=''\n",
    "    bold_phrases_indexed={}\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.pdf'):\n",
    "            print(file)\n",
    "            global file_count\n",
    "            file_count+=1\n",
    "            \n",
    "            print(\"COUNT\",file_count)\n",
    "            try:\n",
    "                pg_no=len(list(extract_pages(os.path.join(path,file))))\n",
    "            except:\n",
    "                pg_no=0\n",
    "            try:\n",
    "                html=convert_pdf(os.path.join(path,file),'html')\n",
    "            except:\n",
    "                continue\n",
    "            soup = BeautifulSoup(html, 'html5lib')\n",
    "            underline_positions=get_underlines(soup)\n",
    "            try:\n",
    "                fontsizes=font_extraction(soup)\n",
    "                file_font_size_mode=mode(fontsizes)\n",
    "            except:\n",
    "                file_font_size_mode=8\n",
    "                \n",
    "            text_para,text_plain_para='',''\n",
    "            text_lis,text_plain_lis,bold_lis,page_list,=[],[],[],[]\n",
    "            bold=False\n",
    "            sub_page_def_list=[]\n",
    "            try:\n",
    "                definitions=def_extraction(os.path.join(path,file))\n",
    "            except:\n",
    "                continue\n",
    "            definitions={key:value for key,value in definitions.items() if key.lower().strip() not in neglect_def}\n",
    "            def_terms=list(definitions.keys())\n",
    "#             print(len(def_terms),'LEN DEFINITIONS')\n",
    "#             print(definitions)\n",
    "            definition_text=''\n",
    "            header_match_object=(0,'',False)\n",
    "            cond_header_match_object=(0,'',False)\n",
    "            ext_header_match_object=(0,'',False)\n",
    "            second_category=False\n",
    "            head_found=False\n",
    "            def_flag=False\n",
    "            cond_head_found=False\n",
    "            ext_head_found=False\n",
    "            single_page_head_found=False\n",
    "            condition_text=''\n",
    "            ext_text=''\n",
    "            excl_text=''\n",
    "            previous_pg_num=[]\n",
    "            def_in_page=[]\n",
    "            cond_single_page_head_found=False\n",
    "            ext_single_page_head_found=False\n",
    "            \n",
    "            font_size=file_font_size_mode\n",
    "\n",
    "            for divs in soup.findAll('div'):\n",
    "                div_text_list=[span.text for span in divs.find_all('span') ]\n",
    "                page_str=str(divs.find_all('a'))\n",
    "                page_num_results=pageNumRE.findall(page_str)\n",
    "                if page_num_results:\n",
    "                    pagenum=page_num_results[0]\n",
    "#                     print(pagenum)\n",
    "                    if pagenum!=previous_pg_num:\n",
    "                        sub_definitions_in_page=[]\n",
    "                    previous_pg_num=pagenum\n",
    "#                     definition_text=''\n",
    "                    \n",
    "                    #only for pdfs\n",
    "                    if(int(pagenum) in [1,2]) and 'Wording' in file and pg_no >20:\n",
    "#                     print('Continued on wording 2')\n",
    "                        continue\n",
    "\n",
    "                    if text_lis:\n",
    "                        text_lis=text_lis[:-1]\n",
    "                        text_para=''.join(text_lis)\n",
    "                        text_plain_lis=text_plain_lis[:-1]\n",
    "                        text_plain_para=''.join(text_plain_lis)\n",
    "                        if(len(re.findall('\\.',text_para))>(len(text_para)/3) or (len(re.findall('\\d',text_para))>80 and int(pagenum)<4 and  pg_no>20 and 'content' in text_para.lower())) :\n",
    "                            print('PASS')\n",
    "                            print(text_plain_para)\n",
    "                            text_para,text_plain_para=' ',' '\n",
    "                            text_lis,text_plain_lis=[],[]\n",
    "                            single_page_head_found=False\n",
    "                            head_found=False\n",
    "                            cond_head_found=False\n",
    "                            ext_head_found=False\n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            bold_lis=clean_pharses(bold_lis)\n",
    "                            pagenum=int(pagenum)-1\n",
    "                            excl_flag,excl_count,excl_pos_lis=excl_extract(text_plain_lis,language)\n",
    "                            cond_flag,cond_count,cond_pos_lis=cond_extract(text_plain_lis,language)\n",
    "                            ext_flag,ext_count,ext_pos_lis=ext_extract(text_plain_lis,language)\n",
    "                            if int(pagenum)==1:\n",
    "                                if head_found or single_page_head_found or excl_count>0:\n",
    "#                                     print(pagenum,True,'!!!')\n",
    "                                    excl_flag=True\n",
    "                                else:\n",
    "                                    excl_flag=False\n",
    "                            else:\n",
    "                                if head_found or single_page_head_found:\n",
    "#                                     print(pagenum,True,'!!!',excl_count)\n",
    "                                    excl_flag=True\n",
    "#                                     print(excl_flag,'EXCL')\n",
    "#                                 else:\n",
    "#                                     print(pagenum,excl_flag,'\\n')\n",
    "# #                             print(excl_text)\n",
    "#                             print(condition_text)\n",
    "                            if cond_head_found or cond_single_page_head_found or cond_count>0:\n",
    "                                cond_flag=True\n",
    "#                                 print('COND','TRUE')\n",
    "                            else:\n",
    "#                                 print('COND','FALSE')\n",
    "                                cond_flag=False\n",
    "    \n",
    "                            if ext_head_found or ext_single_page_head_found or ext_count>0:\n",
    "                                ext_flag=True\n",
    "#                                 print('EXT','TRUE',pagenum)\n",
    "                            else:\n",
    "#                                 print('EXT','FALSE',pagenum)\n",
    "                                ext_flag=False\n",
    "#                             def_in_page=[{'name':k,'text':v[0] for k,v in definitions.items() if v[1]=pagenum}]\n",
    "                            for term,defs in definitions.items():\n",
    "#                                 print(defs[1])\n",
    "                                if defs[1]==pagenum:  ###defs[1] is the pagenumber\n",
    "                                    definition_text=definition_text+'          '+term+' '+defs[0]\n",
    "                                    def_in_page.append({'name':term,'text':defs[0]})\n",
    "#                                     def_in_page{'name':}\n",
    "#                             print(definition_text)\n",
    "                            if definition_text:\n",
    "                                def_flag=True\n",
    "                            \n",
    "                            if def_flag:\n",
    "                                def_search_flag=True\n",
    "#                                 print('DEF','TRUE')\n",
    "                            else:\n",
    "#                                 print('DEF', 'FALSE')\n",
    "                                def_search_flag=False   \n",
    "                            \n",
    "                            if pg_no>2:\n",
    "                                endorsements=False\n",
    "                            else:\n",
    "                                endorsements=True\n",
    "#                             print(endorsements,'ENDORSEMENTS!!!')\n",
    "                            if text_para in local_indexed.keys():\n",
    "                                text_para=text_para+' '\n",
    "                            \n",
    "                            local_indexed[text_para]=(file,pagenum,bold_lis,text_plain_para,excl_flag,excl_count,excl_pos_lis,subdir,sub_page_def_list,country,language,definition_text,def_search_flag,endorsements,excl_text,condition_text,cond_flag,cond_count,cond_pos_lis,def_in_page,ext_text,ext_flag,ext_count,ext_pos_lis)\n",
    "#                             print(sub_page_def_list,'DEF LIST!!!!!1')\n",
    "                            sub_page_def_list=[]\n",
    "                            page_list.append(pagenum)\n",
    "                            text_para,text_plain_para='',''\n",
    "                            text_lis,text_plain_lis,bold_lis=[],[],[]\n",
    "                            definition_text=''\n",
    "                            cond_single_page_head_found=False\n",
    "                            ext_single_page_head_found=False\n",
    "                            condition_text=''\n",
    "                            ext_text=''\n",
    "                            excl_text=''\n",
    "                            single_page_head_found=False\n",
    "                            def_in_page=[]\n",
    "                            def_flag=False\n",
    "                for span in divs.find_all('span'):\n",
    "\n",
    "                    bold=False\n",
    "                    upper=False\n",
    "                    bullet=False\n",
    "                    def_flag=False\n",
    "                    span_position=div_text_list.index(span.text)\n",
    "                    \n",
    "                    if \"Bold\" in str(span) or 'CIDFont+F3' in str(span):\n",
    "                        bold_lis.append(span.text)\n",
    "                        bold=True\n",
    "                    if span.text.isupper():\n",
    "                        upper=True\n",
    "                    font_family_match=re.findall(r\"font-family: b'(.*)';\",str(span))\n",
    "                    if font_family_match:\n",
    "                        font_family=font_family_match[0]\n",
    "                    else:\n",
    "                        font_family=''\n",
    "#                     print(font_family)\n",
    "                    font_size_match=re.findall(r'font-size:(.*)px\">',str(span))\n",
    "                    if font_size_match:\n",
    "                        font_size=int(font_size_match[0])\n",
    "                    if pointsRE_heading.findall(span.text):\n",
    "                        bullet=True\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    y_cord_list=y_cordinate_Re.findall(str(divs))\n",
    "                    if y_cord_list:\n",
    "                        y_cord=int(y_cord_list[0])\n",
    "                    underlined_text=check_underline(underline_positions,y_cord)\n",
    "                    if span.text.split('\\n')[0]!='' and span.text.split('\\n')[0]!=' ':\n",
    "                        head_check_text=span.text.split('\\n')[0]\n",
    "                    elif len(span.text.split('\\n'))>1:\n",
    "                        head_check_text=span.text.split('\\n')[1]\n",
    "                    else:\n",
    "                        head_check_text=''\n",
    "##############################3  \n",
    "                    if str(previous_span).endswith('<br/></span>'):\n",
    "                        span_position=0\n",
    "                    ##################INSERT FUNCTION####################z\n",
    "                    #EXCLUSION \n",
    "                    if  span_position==0 and (bold or font_size>= file_font_size_mode +2 ) and (font_size> file_font_size_mode or upper) and 5<len(head_check_text.strip())<80:                    \n",
    "                        if exclusion_check(head_check_text,language) and not head_found: \n",
    "                            head_found=True\n",
    "                            header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "                            First_category=True\n",
    "                            second_category=False\n",
    "                            single_page_head_found=True\n",
    "#                             print('FOUND Heading for exclusion ...',span.text)\n",
    "                        \n",
    "                    \n",
    "                        elif ((font_size,font_family,bold,upper,bullet)==header_match_object  or font_size>header_match_object[0] ) and head_found and not exclusion_check(head_check_text,language):\n",
    "                            head_found=False\n",
    "                            single_page_head_found=True\n",
    "#                             print('FOUND Closure for exclusion ...',span.text)\n",
    "#                     if 'CLAIMS CONDITIONS' in span.text:\n",
    "#                         print ('^^^^^^^',(font_size,font_family,bold,upper,bullet),header_match_object, font_size>header_match_object[0] , head_found , not exclusion_check(head_check_text,language))\n",
    "#                         print(pointsRE_heading.findall(span.text))\n",
    "                    words_title=[word.istitle() for word in head_check_text.split() if stopword_check(word,head_check_text,language) and not word.isdigit() ]\n",
    "                    if span_position==0 and all(words_title) and head_check_text.strip() not in definitions and  (len(words_title) >1 or underlined_text )and 5<len(head_check_text.strip())<80 :\n",
    "#                         print('MMMMMMMMARARARA',span.text)\n",
    "\n",
    "                        if exclusion_check(head_check_text,language) and  not head_found:\n",
    "                            head_found=True\n",
    "                            second_category=True\n",
    "                            First_category=False\n",
    "                            single_page_head_found=True\n",
    "                            header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "#                             print(\"FOUND heading type2 ....\",span.text)\n",
    "                        elif head_found and not exclusion_check(head_check_text,language) and second_category and ((font_size,font_family,bold,upper,bullet)==header_match_object or font_size > header_match_object[0] ) :\n",
    "                            head_found=False\n",
    "                            second_category=False\n",
    "                            single_page_head_found=True\n",
    "#                             print('FOUND closure type 2',span.text)\n",
    "                    if head_found:\n",
    "                        excl_text=excl_text+' '+span.text     \n",
    "                     \n",
    "                    #CONDITIONS\n",
    "#                     if 'Condition' in span.text:\n",
    "# #                         print('text:',str(span.text),'head check',head_check_text,'length of head check',len(head_check_text))\n",
    "# #                         print(span_position==0,(bold or font_size>= file_font_size_mode +2 ), (font_size> file_font_size_mode or upper), 5<len(head_check_text.strip())<80)\n",
    "# #                         print(font_size,file_font_size_mode,cond_head_found,criteria_check(head_check_text,language))\n",
    "                    if  span_position==0 and (bold or font_size>= file_font_size_mode +2 ) and (font_size> file_font_size_mode or upper) and 5<len(head_check_text.strip())<80:                    \n",
    "                        if criteria_check(head_check_text,language) and not cond_head_found: \n",
    "                            cond_head_found=True\n",
    "                            cond_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "                            cond_First_category=True\n",
    "                            cond_second_category=False\n",
    "                            cond_single_page_head_found=True\n",
    "#                             print('FOUND Heading for condition...',span.text)\n",
    "                    \n",
    "                        elif ((font_size,font_family,bold,upper,bullet)==cond_header_match_object or font_size>cond_header_match_object[0] ) and not criteria_check(head_check_text,language)  and cond_head_found:\n",
    "                            cond_head_found=False\n",
    "                            cond_single_page_head_found=True\n",
    "#                             print('FOUND Closure for condition ...',span.text)\n",
    "                \n",
    "                    \n",
    "                    cond_words_title=[word.istitle() for word in head_check_text.split() if stopword_check(word,head_check_text,language) and not word.isdigit() ]\n",
    "                    if span_position==0 and all(cond_words_title) and head_check_text.strip() not in definitions and  (len(words_title) >1 or underlined_text )and 5<len(head_check_text.strip())<80 :\n",
    "#                         print('MMMMMMMMARARARA',span.text)\n",
    "\n",
    "                        if criteria_check(head_check_text,language) and  not cond_head_found:\n",
    "                            cond_head_found=True\n",
    "                            cond_second_category=True\n",
    "                            cond_First_category=False\n",
    "                            cond_single_page_head_found=True\n",
    "                            cond_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "#                             print(\"FOUND heading type2 ....\",span.text)\n",
    "                        elif cond_head_found and not criteria_check(head_check_text,language) and cond_second_category and ((font_size,font_family,bold,upper,bullet)==cond_header_match_object or font_size > cond_header_match_object[0] ) :\n",
    "                            cond_head_found=False\n",
    "                            cond_second_category=False\n",
    "                            cond_single_page_head_found=True\n",
    "#                             print('FOUND closure type 2',span.text)\n",
    "\n",
    "                    if cond_head_found==True:\n",
    "                        condition_text=condition_text+' '+span.text\n",
    "                            \n",
    "                    \n",
    "                    #####EXTENSIONS\n",
    "                    #CONDITIONS\n",
    "#                     if 'Extension' in span.text:\n",
    "#                         print(span.text)\n",
    "#                         print('text:',str(span.text),'head check',head_check_text,'length of head check',len(head_check_text))\n",
    "#                         print(span_position==0,(bold or font_size>= file_font_size_mode +2 ), (font_size> file_font_size_mode or upper), 5<len(head_check_text.strip())<80)\n",
    "#                         print(font_size,file_font_size_mode,cond_head_found,criteria_check(head_check_text,language))\n",
    "                    if  span_position==0 and (bold or font_size>= file_font_size_mode +2 ) and (font_size> file_font_size_mode or upper) and 5<len(head_check_text.strip())<80:                    \n",
    "                        if ext_check(head_check_text,language) and not ext_head_found: \n",
    "                            ext_head_found=True\n",
    "                            ext_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "                            ext_First_category=True\n",
    "                            ext_second_category=False\n",
    "                            ext_single_page_head_found=True\n",
    "#                             print('FOUND Heading for ext...',span.text)\n",
    "                    \n",
    "                        elif ((font_size,font_family,bold,upper,bullet)==ext_header_match_object or font_size>ext_header_match_object[0] ) and not ext_check(head_check_text,language)  and ext_head_found:\n",
    "                            ext_head_found=False\n",
    "                            ext_single_page_head_found=True\n",
    "#                             print('FOUND Closure for ext ...',span.text)\n",
    "                \n",
    "                    \n",
    "                    ext_words_title=[word.istitle() for word in head_check_text.split() if stopword_check(word,head_check_text,language) and not word.isdigit() ]\n",
    "                    if span_position==0 and all(ext_words_title) and head_check_text.strip() not in definitions and  (len(words_title) >1 or underlined_text )and 5<len(head_check_text.strip())<80 :\n",
    "#                         print('MMMMMMMMARARARA',span.text)\n",
    "\n",
    "                        if ext_check(head_check_text,language) and  not ext_head_found:\n",
    "                            ext_head_found=True\n",
    "                            ext_second_category=True\n",
    "                            ext_First_category=False\n",
    "                            ext_single_page_head_found=True\n",
    "                            ext_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "#                             print(\"FOUND heading type2 ext....\",span.text)\n",
    "                        elif ext_head_found and not ext_check(head_check_text,language) and ext_second_category and ((font_size,font_family,bold,upper,bullet)==ext_header_match_object or font_size > ext_header_match_object[0] ) :\n",
    "                            ext_head_found=False\n",
    "                            ext_second_category=False\n",
    "                            ext_single_page_head_found=True\n",
    "#                             print('FOUND closure type 2 ext',span.text)\n",
    "# \n",
    "                    if ext_head_found==True:\n",
    "                        ext_text=ext_text+' '+span.text\n",
    "                            \n",
    "    #                     else:\n",
    "###############################             \n",
    "                        \n",
    "                    if font_size_match:\n",
    "                        if font_size<=file_font_size_mode-1:\n",
    "                            text_lis.append(' ')\n",
    "                    text=span.text.replace('\\n',' ')\n",
    "                    text_plain=text.replace('\\n',' ')\n",
    "                    for term in def_terms:\n",
    "                        if term.lower().strip() in text.lower() and definitions[term][0].strip() not in text and term not in sub_definitions_in_page:\n",
    "#                             def_page.append(definitions[term])\n",
    "#                             print(pagenum)\n",
    "                            src_str  = re.compile(re.escape(term), re.IGNORECASE)\n",
    "                            text=src_str.sub('###{}@@{}%%%'.format(term,definitions[term][0]),text)\n",
    "#                             print(text,'\\n',term,'TERM!!!!@#$%^&*&^%#$%^&')\n",
    "                            sub_definitions_in_page.append(term)\n",
    "                            sub_page_def_list.append({'name':term,'text':definitions[term][0]})\n",
    "#                             print(definitions_in_page)\n",
    "                            break\n",
    "#                     print(definitions_in_page)\n",
    "\n",
    "                    \n",
    "                    text_lis.append(text)\n",
    "                    text_plain_lis.append(text_plain)\n",
    "                    previous_span=span\n",
    "                    \n",
    "\n",
    "            if text_lis:\n",
    "                if(int(pagenum) in [1,2]) and 'Wording' in file and pg_no >20:\n",
    "#                     print('Continued on wording 2')\n",
    "                    continue\n",
    "\n",
    "                text_lis=text_lis[:-1]\n",
    "                text_para=''.join(text_lis)\n",
    "                text_plain_lis=text_plain_lis[:-1]\n",
    "                text_plain_para=''.join(text_plain_lis)\n",
    "                if(len(re.findall('\\.',text_para))>(len(text_para)/3) or (len(re.findall('\\d',text_para))>80 and int(pagenum)<4 and pg_no>20 and 'content' in text_para.lower())) :\n",
    "                    print('PASS')\n",
    "                    print(text_plain_para)\n",
    "                    text_para,text_plain_para=' ',' '\n",
    "                    text_lis,text_plain_lis=[],[]\n",
    "                    single_page_head_found=False\n",
    "                    head_found=False\n",
    "                    cond_head_found=False\n",
    "                    ext_head_found=False\n",
    "                    continue\n",
    "                else:\n",
    "                    bold_lis=clean_pharses(bold_lis)\n",
    "                    if pagenum in page_list and pagenum!='1':\n",
    "                        pagenum=int(pagenum)+1\n",
    "                    excl_flag,excl_count,excl_pos_lis=excl_extract(text_plain_lis,language)\n",
    "                    cond_flag,cond_count,cond_pos_lis=cond_extract(text_plain_lis,language)\n",
    "                    ext_flag,ext_count,ext_pos_lis=ext_extract(text_plain_lis,language)\n",
    "                    if int(pagenum)==1:\n",
    "                        if head_found or single_page_head_found or excl_count>0:\n",
    "#                             print(pagenum,True,'!!!',excl_count)\n",
    "                            excl_flag=True\n",
    "                        else:\n",
    "                            excl_flag=False\n",
    "                    else:\n",
    "                        if head_found or single_page_head_found:\n",
    "#                             print(pagenum,True,'!!!')\n",
    "                            excl_flag=True\n",
    "#                             print(excl_flag,'EXCL')\n",
    "                        \n",
    "                    #####################\n",
    "                    if cond_head_found or cond_single_page_head_found or cond_count>0:\n",
    "                        cond_flag=True\n",
    "#                         print('COND','TRUE')\n",
    "                    else:\n",
    "                        cond_flag=False\n",
    "                    \n",
    "                    if ext_head_found or ext_single_page_head_found or ext_count>0:\n",
    "                        ext_flag=True\n",
    "#                         print('EXT','TRUE',pagenum)\n",
    "                    else:\n",
    "#                         print('EXT','FALSE',pagenum)\n",
    "                        ext_flag=False\n",
    "#                         print('COND','FALSE')\n",
    "#                     page_def_list.append({'name':term,'text':definitions[term][0]}\n",
    "#                     def_in_page=[{'name':k,'text':v[0]} for k,v in definitions.items() if v[1]==pagenum]  \n",
    "#                     print(def_in_page,'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                    for term,defs in definitions.items():\n",
    "                        if defs[1]==pagenum:  ###defs[1] is the pagenumber\n",
    "                            definition_text=definition_text+'          '+term+' '+defs[0]\n",
    "                            def_in_page.append({'name':term,'text':defs[0]})        \n",
    "#                     print(definition_text)\n",
    "                    if definition_text:\n",
    "                        def_flag=True\n",
    "\n",
    "                    if def_flag:\n",
    "                        def_search_flag=True\n",
    "#                         print('DEF','TRUE')\n",
    "                    else:\n",
    "                        def_search_flag=False\n",
    "#                         print('DEF','FALSE')\n",
    "                    \n",
    "                    if pg_no>2:\n",
    "                        endorsements=False\n",
    "                    else:\n",
    "                        endorsements=True\n",
    "#                     print(endorsements,'ENDORSEMENTS!!!')        \n",
    "                    if text_para in local_indexed.keys():\n",
    "                        text_para=text_para+' '\n",
    "                    local_indexed[text_para]=(file,pagenum,bold_lis,text_plain_para,excl_flag,excl_count,excl_pos_lis,subdir,sub_page_def_list,country,language,definition_text,def_search_flag,endorsements,excl_text,condition_text,cond_flag,cond_count,cond_pos_lis,def_in_page,ext_text,ext_flag,ext_count,ext_pos_lis)\n",
    "#                     print(sub_page_def_list,'DEF LIST!!!!!1')\n",
    "                    sub_page_def_list=[]\n",
    "                    text_para,text_plain_para='','' \n",
    "                    text_plain_lis,text_lis,bold_lis=[],[],[]\n",
    "                    definition_text=''\n",
    "                    single_page_head_found=False\n",
    "                    cond_single_page_head_found=False\n",
    "                    ext_single_page_head_found=False\n",
    "                    condition_text=''\n",
    "                    ext_text=''\n",
    "                    excl_text=''\n",
    "                    def_in_page=[]\n",
    "                    def_flag=False\n",
    "    return(local_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_pdf_path(path):\n",
    "    file_list=os.listdir(path)\n",
    "    for file in file_list:\n",
    "        raw_filename=file[:file.rfind(\".\")]\n",
    "        pdf_filename=raw_filename+'.pdf'\n",
    "#         print(raw_filename,pdf_filename,'FILE!!')\n",
    "        if pdf_filename in file_list:\n",
    "            print('continued!!!!!!!!!!!!!')\n",
    "            continue\n",
    "        if file.endswith('doc') or file.endswith('docx'):\n",
    "            print('processing!!!')\n",
    "            wordToPdf(os.path.join(path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootDir='V://COG//AU- Property//'\n",
    "# for dirname,subdirlist,filelist in os.walk(rootDir):\n",
    "#     print(dirname)\n",
    "#     for subdir in subdirlist:\n",
    "#         path=os.path.join(dirname,subdir)\n",
    "# #         doc_pdf_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other Carrier Wording SUBDIR!!!!!!!!!!!!!\n",
      "Broker Wording SUBDIR!!!!!!!!!!!!!\n",
      "Mining Operation Activities Exclusion (SC_Vertex_EXL_0915.MOA.1).pdf\n",
      "COUNT 1\n",
      "Tour Operators Exclusion (SC_Vertex_UMB_0915.TOUROE.1).pdf\n",
      "COUNT 2\n",
      "MTBE Exclusion (SC_GA_UMB_V3.MTBE.2).pdf\n",
      "COUNT 3\n",
      "Oil Industry Exclusion (SC_Vertex_GPL_1018.OIE.1).pdf\n",
      "COUNT 4\n",
      "Chubb02-148-1020 Gallagher Chubb Combined General Legal Liability Quote.pdf\n",
      "COUNT 5\n",
      "PASS\n",
      "          Contents  Important Notices........................................................................................ 3  Financial Strength Rating ............................................................................. 4  Combined General Legal Liability Insurance Quote ......................................... 5  Endorsements ............................................................................................. 6  Privacy Statement ........................................................................................ 8  About Chubb in Australia.............................................................................10  Contact Chubb ............................................................................................10  About Gallagher..........................................................................................10    Combined  General Legal Liability Insurance  Quote,  Australia. Published  10/2020.   ©2020 Chubb  Insurance Australia Limited.  Chubb®,  its logos, and Chubb.Insured.℠  are protected  trademarks of Chubb.  \n",
      "Nuclear Exclusion (SC_Vertex_EXL_0915.NUCLR.1).pdf\n",
      "COUNT 6\n",
      "Hazardous Stunts or Special Effects Exclusion (SC_Vertex_EXL_0915.HSSEE.1).pdf\n",
      "COUNT 7\n",
      "Mould Fungus Exclusion (SC_MMA_CGL_V3.0_MFE.1).pdf\n",
      "COUNT 8\n",
      "Total Communicable Disease Schedule Activities Exclusion (SC_GA_EXCESS_CGL_V3.TCDESA.1).pdf\n",
      "COUNT 9\n",
      "Animal Disease Exclusion (SC_Vertex_EXL_0915.ADE.1).pdf\n",
      "COUNT 10\n",
      "Treatmemt Risk Exclusion (SC_GA_EXCESS_CGL_V3.TRE.1).pdf\n",
      "COUNT 11\n",
      "Labour Hire Exclusion (SC_GA_EXCESS_CGL_V3.LHE.1).pdf\n",
      "COUNT 12\n",
      "Winery Contract Activities Exclusion (SC_Chubb02930217.WCAE.1).pdf\n",
      "COUNT 13\n",
      "Silica Exclusion (SC_WP_GPL_0911.SE.1).pdf\n",
      "COUNT 14\n",
      "Total Communicable Disease Exclusion (SC_Vertex_GPL_1018.TCDE.1).pdf\n",
      "COUNT 15\n",
      "TSE and BSE Exclusion (SC_WP_GPL_0911.TSEBSE.1).pdf\n",
      "COUNT 16\n",
      "DIC DIL Endorsement (SC_Chubb02940217.DICDIL.1).pdf\n",
      "COUNT 17\n",
      "Contractual Liability (Total) Exclusion (SC_Chubb02930217.CLTE.1).pdf\n",
      "COUNT 18\n",
      "Cellular Phone Radiation Exclusion (SC_Chubb21120617.CPRE.1).pdf\n",
      "COUNT 19\n",
      "Contract Works Exclusion (SC_Vertex_EXL_0915.CWE.1).pdf\n",
      "COUNT 20\n",
      "Mould Fungus Exclusion (SC_Vertex_UMB_0915.MFE.1).pdf\n",
      "COUNT 21\n",
      "8. Hold Harmless Endorsement (SC_WP_GPL_0911.HH.1).pdf\n",
      "COUNT 22\n",
      "Hotel Operations Exclusion (SC_Vertex_GPL_1018.HOTOE.1).pdf\n",
      "COUNT 23\n",
      "Contractual Liability (Nominated Contracts) Endorsement (SC_Chubb02940217.CLSPEE.1).pdf\n",
      "COUNT 24\n",
      "New Zealand Exemplary Damages Endorsement (SC_Vertex_GPL_1018.NZEDE.1).pdf\n",
      "COUNT 25\n",
      "Intoxication Exclusion (SC_Vertex_EXL_0915.INTOXE.1).pdf\n",
      "COUNT 26\n",
      "Subsidence Exclusion (SC_Vertex_GPL_1018.SUBE.1).pdf\n",
      "COUNT 27\n",
      "Participation Exclusion (SC_Chubb02930217.PARTE.1).pdf\n",
      "COUNT 28\n",
      "Chemicals Exclusion (SC_Vertex_UMB_0915.CHEME.1).pdf\n",
      "COUNT 29\n",
      "Principals Indemnity (Unspecified) Endorsement (SC_Chubb02940217.PRIUNSP.1).pdf\n",
      "COUNT 30\n",
      "Airside Exclusion (SC_MMA_CGL_V3.0.ASE.1).pdf\n",
      "COUNT 31\n",
      "Specified Insured Exclusion (SC_ Chubb02940217.SINY.1).pdf\n",
      "COUNT 32\n",
      "Spray Drift Exclusion (SC_GA_UMB_V3.SDE.1).pdf\n",
      "COUNT 33\n",
      "Specified Products Exclusion (SC_GA_EXCESS_CGL_V3.SPEP.1).pdf\n",
      "COUNT 34\n",
      "Tobacco Exclusion (SC_WP_GPL_0911.TOBE.2).pdf\n",
      "COUNT 35\n",
      "GMO Exclusion (SC_MMA_CGL_V3.0_GMO.1).pdf\n",
      "COUNT 36\n",
      "MMA Combined Liability Policy (MMA_CGL_V3.0).pdf\n",
      "COUNT 37\n",
      "Treatment Risk (Prescription) Exclusion (SC_Chubb02930217.TREATPREE.1).pdf\n",
      "COUNT 38\n",
      "EMR Endorsement (SC_MMA_CGL_V3.0.EMR.1).pdf\n",
      "COUNT 39\n",
      "Animal Disease Exclusion (SC_Chubb21120617.ADE.1).pdf\n",
      "COUNT 40\n",
      "Failure to Supply Exclusion (SC_WP_GPL_0911.FSE.1).pdf\n",
      "COUNT 41\n",
      "Latex Exclusion (SC_Vertex_UMB_0915.LTXE.1).pdf\n",
      "COUNT 42\n",
      "Intoxication Exclusion (SC_GA_UMB_V3.INTOXE.1).pdf\n",
      "COUNT 43\n",
      "Molestation Exclusion (SC_Chubb02940217.MOL.1).pdf\n",
      "COUNT 44\n",
      "Total Communicable Disease Exclusion Scheduled Activities (SC_Chubb21120617.TCDESA.1).pdf\n",
      "COUNT 45\n",
      "APPROVED MMA DIC CCC Endorsement (SC_GA_EXCESS_CGL_V3.AMMADICCCCE.1).pdf\n",
      "COUNT 46\n",
      "Efficacy (Nominated Products) Exclusion (SC_ MMA_CGL_V3.0_NPEE.1).pdf\n",
      "COUNT 47\n",
      "Change of Business Description Endorsement (SC_MMA_CGL_V3.0_CHBD.1).pdf\n",
      "COUNT 48\n",
      "Latex Exclusion (SC_GA_EXCESS_CGL_V3.LTXE.1).pdf\n",
      "COUNT 49\n",
      "7. Contractual Agreements Endorsement (SC_WP_GPL_0911.CA.1).pdf\n",
      "COUNT 50\n",
      "TSE and BSE Exclusion (SC_Chubb02940217.TSEBSE.1).pdf\n",
      "COUNT 51\n",
      "Avian Flu Exclusion (SC_GA_EXCESS_CGL_V3.AVFE.1).pdf\n",
      "COUNT 52\n",
      "Property in Your Care Custody and Control DIC ISR Endorsement (SC_Chubb21120617.CCCD.1).pdf\n",
      "COUNT 53\n",
      "Products Liability Exclusion (SC_Vertex_EXL_0915.PROD.1).pdf\n",
      "COUNT 54\n",
      "Tobacco Exclusion (SC_Chubb21120617.TOBE.2).pdf\n",
      "COUNT 55\n",
      "Spray Drift Exclusion (SC_GA_EXCESS_CGL_V3.SDEX.1).pdf\n",
      "COUNT 56\n",
      "Specified Activities Exclusion (SC_WP_GPL_0911.SA.1).pdf\n",
      "COUNT 57\n",
      "Contractor Subcontractor Labour Hire Personnel Injury Exclusion (SC_Chubb02930217.CSLHP.1).pdf\n",
      "COUNT 58\n",
      "Principals Indemnity (Specified) Endorsement (SC_Chubb02940217.PRISP.1).pdf\n",
      "COUNT 59\n",
      "Animal Disease Exclusion (SC_MMA_CGL_V3.0_ADE.2).pdf\n",
      "COUNT 60\n",
      "Austbrokers Broadform Liability Insurance Binder Template.pdf\n",
      "COUNT 61\n",
      "Welding and Hotworks Exclusion (SC_Chubb02930217.WHWE.1).pdf\n",
      "COUNT 62\n",
      "Underground Services Exclusion Write Back (SC_GA_UMB_V3.USE.2).pdf\n",
      "COUNT 63\n",
      "Welding and Hotworks Exclusion (SC_GA_UMB_V3.WHWE.1).pdf\n",
      "COUNT 64\n",
      "Products Liability (Specific Ingredients) Exclusion (SC_GA_UMB_V3.PLSI.1).pdf\n",
      "COUNT 65\n",
      "Sexual Abuse, Harassment and Molestation Exclusion (SC_MMA_CGL_V3.0.SAHME.1).pdf\n",
      "COUNT 66\n",
      "Property in Your Care, Custody or Control (Total) Exclusion (SC_Chubb21120617.CCCTE.1).pdf\n",
      "COUNT 67\n",
      "Total Communicable Disease Scheduled Activites Exclusion (SC_Vertex_EXL_0915.TCDESA.1).pdf\n",
      "COUNT 68\n",
      "Loss of Electronic Data Exclusion (SC_Vertex_GPL_1018.LOEDE.1).pdf\n",
      "COUNT 69\n",
      "Advertising liability Exclusion (SC_GA_UMB_V3.ALE.1).pdf\n",
      "COUNT 70\n",
      "Pharmaceuticals (Prescription) Exclusion (SC_MMA_CGL_V3.0_PRMP.1).pdf\n",
      "COUNT 71\n",
      "Extra Territorial Workers Compensation Insurance Endorsement (SC_MMA_CGL_V3.0_ETWCI.1).pdf\n",
      "COUNT 72\n",
      "Money and Securities Exclusion (SC_Vertex_GPL_1018.MONSECE.1).pdf\n",
      "COUNT 73\n",
      "Defamation and Privacy Exclusion (SC_Chubb02940217.DEFPRIE.1).pdf\n",
      "COUNT 74\n",
      "Money Exclusion (SC_Vertex_GPL_1018.MONE.1).pdf\n",
      "COUNT 75\n",
      "Participation Exclusion (SC_Vertex_GPL_1018.PARTE.1).pdf\n",
      "COUNT 76\n",
      "Steadfast (SCTP) General Public and Products Liability Insurance Binder Template.pdf\n",
      "COUNT 77\n",
      "Molestation Exclusion (SC_Chubb02930217.MOL.1).pdf\n",
      "COUNT 78\n",
      "Molestation Endorsement (SC_ Chubb02940217.MOLEE.1).pdf\n",
      "COUNT 79\n",
      "Contractual Liability (Nominated Contract) Endorsement (SC_GA_EXCESS_CGL_V3.CLNCE.1).pdf\n",
      "COUNT 80\n",
      "Lead Exclusion (SC_Chubb02940217.LDE.1).pdf\n",
      "COUNT 81\n",
      "Change of Deductible Endorsement (SC_WP_GPL_0911.CDE.1).pdf\n",
      "COUNT 82\n",
      "Personal Injury to Contractors Underground (SC_VERTX_EXL_0915.PICUG.1).pdf\n",
      "COUNT 83\n",
      "Labour Hire Exclusion (SC_MMA_CGL_V3.0_LABOURE.2).pdf\n",
      "COUNT 84\n",
      "Money and Securities Exclusion (SC_Vertex_EXL_0915.MONSECE.1).pdf\n",
      "COUNT 85\n",
      "Aircraft Charter Exclusion (SC_GA_EXCESS_CGL_V3.ACE.1).pdf\n",
      "COUNT 86\n",
      "Legionnaires Disease Exclusion (SC_MMA_CGL_V3.0_LEGD.1).pdf\n",
      "COUNT 87\n",
      "Advertising Injury Exclusion (SC_Vertex_GPL_1018.ADVER.1).pdf\n",
      "COUNT 88\n",
      "Hotel Operations Exclusion (SC_GA_EXCESS_CGL_V3.HOTOE.2).pdf\n",
      "COUNT 89\n",
      "Participation Exclusion (SC_MMA_CGL_V3.0_PRTE.1).pdf\n",
      "COUNT 90\n",
      "Rip and Tear Endorsement (SC_Chubb02930217.RPTRE.1).pdf\n",
      "COUNT 91\n",
      "PCB Exclusion (SC_Chubb02940217.PCBE.1).pdf\n",
      "COUNT 92\n",
      "Advertising Injury Exclusion (SC_MMA_CGL_V3.0_AIE.1).pdf\n",
      "COUNT 93\n",
      "Australian Standards Exclusion (SC_Vertex_GPL_1018.ASE.1).pdf\n",
      "COUNT 94\n",
      "Injury to Contractor Exclusion (SC_Vertex_GPL_1018.ICONE.1).pdf\n",
      "COUNT 95\n",
      "Australian Compliance Condition (SC_MMA_CGL_V3.0.ACC.1).pdf\n",
      "COUNT 96\n",
      "WillPlace General and Products Liability Insurance Policy.pdf\n",
      "COUNT 97\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "file_count=0\n",
    "rootDir='Documents/AU-Casualty/'\n",
    "country='australia'\n",
    "local_indexed={}\n",
    "language='english'\n",
    "if 'spanish' in rootDir:\n",
    "    language='spanish'\n",
    "for dirname,subdirlist,filelist in os.walk(rootDir):\n",
    "    for subdir in subdirlist:\n",
    "        print(subdir, 'SUBDIR!!!!!!!!!!!!!')\n",
    "        path=os.path.join(dirname,subdir)    \n",
    "        local_indexed_subdir=create_local_index(path,subdir,language,country)\n",
    "        local_indexed.update(local_indexed_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(local_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "\n",
    "cosmo_endpoint=\"https://nf-poc-cdb-sql.documents.azure.com\"\n",
    "cosmo_key=\"iEcEfrxYe0Fm9QtoxDrOpLvGsfzjowwybULlWT9Uz4XxV4RmOIAnRuLdgRFUu1LPU5Vwk3UGivRrPrxnk7083w==\"\n",
    "client = CosmosClient(cosmo_endpoint, cosmo_key)\n",
    "database=client.get_database_client('policy-analysis')\n",
    "container=database.get_container_client('casualty-wordings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in local_indexed.items():\n",
    "    doc={}\n",
    "    doc['text']=key\n",
    "    doc['page']=int(value[1])\n",
    "    if value[4]==True or value[0].find('Exclusion')!=-1:\n",
    "        doc['IsExclusion']=True\n",
    "    else:\n",
    "        doc['IsExclusion']=False\n",
    "    doc['doc_name']=value[0].replace('.pdf','')\n",
    "    doc['id']=doc['doc_name']+' '+str(doc['page'])\n",
    "    doc['bold_phrases']=value[2]\n",
    "    doc['plain_text']=value[3]\n",
    "    doc['excl_count']=value[5]\n",
    "    doc['excl_pos']=value[6]\n",
    "    doc['folder']=value[7]\n",
    "    doc['definitions']=[{'name':key,'text':value} for key,value in re.findall('###(.*?)@@(.*?)%%%',doc['text'])]\n",
    "    doc['country']=value[9]\n",
    "    doc['language']=value[10]\n",
    "    doc['definition_text']=value[11]\n",
    "#     print(doc['definition_text'],'\\n\\n')\n",
    "    if doc['definition_text']:\n",
    "        doc['definition_flag']=True\n",
    "    else:\n",
    "        doc['definition_flag']=False\n",
    "    if value[14]==True or 'endorsement' in value[0].lower(): \n",
    "        doc['endorsements']=True\n",
    "    else:\n",
    "        doc['endorsements']=value[13]\n",
    "    doc['excl_text']=value[14]\n",
    "    doc['cond_text']=value[15]\n",
    "    doc['cond_flag']=value[16]\n",
    "    doc['cond_count']=value[17]\n",
    "    doc['cond_pos']=value[18]\n",
    "    doc['definitions_in_page']=value[19]\n",
    "    doc['ext_text']=value[20]\n",
    "    doc['ext_flag']=value[21]\n",
    "    doc['ext_count']=value[22]\n",
    "    doc['ext_pos']=value[23]\n",
    "    \n",
    "    try:       \n",
    "        container.create_item(body=doc)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('added to cosmos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
