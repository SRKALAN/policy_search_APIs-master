{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install comtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app/RB/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import uuid\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords\n",
    "from pdf2txt import convert_pdf\n",
    "# !pip install tika\n",
    "import tika\n",
    "from definitions_v3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsRE_heading=re.compile('(?:\\s*\\([a-z]{1,3}\\)|[A-Z]{1}\\s+[a-zA-Z0-9_\\s]{5})')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "y_cordinate_Re=re.compile('top:(\\d{1,5})px')\n",
    "height_re=re.compile('height:(\\d{1,5})px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_underline(positions,y_cord_text):\n",
    "    positions=[i for i in positions if y_cord_text<=i< y_cord_text+16]\n",
    "    if positions:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_no(soup):\n",
    "    total_items = int(soup.find('span', id='NewReleases_total').text)\n",
    "    items_per_page = int(soup.find('span', id='NewReleases_end').text)\n",
    "    return round(total_items/items_per_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "\n",
    "# print(len(list(extract_pages('test/Chubb16-250-1019 Chubb EBM Business Pack Product Disclosure Statement (PDS) and Policy Wording.pdf'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_underlines(soup):\n",
    "    positions=[]\n",
    "    for span in soup.find_all('span'):\n",
    "        y_cord_list=y_cordinate_Re.findall(str(span))\n",
    "        if y_cord_list:\n",
    "            y_cord=int(y_cord_list[0])\n",
    "        else:\n",
    "            continue\n",
    "        style=\"position:absolute; border: black 1px solid\" in str(span)\n",
    "        height_px_li=height_re.findall(str(span))\n",
    "        if height_px_li:\n",
    "            height_px=height_px_li[0]\n",
    "        else:\n",
    "            continue\n",
    "        height=int(height_px)<15\n",
    "        if all([style,height]):\n",
    "            positions.append(y_cord)\n",
    "    return list(set(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_extract(text_list,language):\n",
    "    ext_flag=False\n",
    "    if language=='english':\n",
    "        ext_lis=['extension']\n",
    "    else:\n",
    "        ext_lis=['extensión','extensiones']\n",
    "   \n",
    "    ext_count=0\n",
    "    position_list=[]\n",
    "    position=0\n",
    "    for text in text_list:\n",
    "        # if any two of 'exclusion ,condition or extension is present in the text, that text is omitted from the logic'\n",
    "        bool_list=['condition' in text.lower(),'exclusion' in text.lower(),'extension' in text.lower()]\n",
    "        if sum(bool_list)>=2:\n",
    "            continue\n",
    "        for i in ext_lis:\n",
    "            for match in re.finditer(i, text.lower()):\n",
    "                position_list.append(position+match.start())               \n",
    "                ext_count+=1\n",
    "        position+=len(text)\n",
    "    if ext_count>=1:\n",
    "        ext_flag=True\n",
    "    return(ext_flag,ext_count,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excl_extract(text_list,language):\n",
    "    exclusion_flag=False\n",
    "    if language=='english':\n",
    "        excl_lis=['exclud','not cover','except','does not mean','not includ','exclusion']\n",
    "    else:\n",
    "        excl_lis=['excepto','excepción','no in cluido','exclusión','excluidos','excluirlo','excluyendo','exclusiones','exclusion','excluyen','excluyentes']\n",
    "   \n",
    "    exclusion_count=0\n",
    "    position_list=[]\n",
    "    position=0\n",
    "    for text in text_list:\n",
    "        # if any two of 'exclusion ,condition or extension is present in the text, that text is omitted from the logic'\n",
    "        bool_list=['condition' in text.lower(),'exclusion' in text.lower(),'extension' in text.lower()]\n",
    "        if sum(bool_list)>=2:\n",
    "            continue\n",
    "        for i in excl_lis:\n",
    "            for match in re.finditer(i, text.lower()):\n",
    "                position_list.append(position+match.start())               \n",
    "                exclusion_count+=1\n",
    "        position+=len(text)\n",
    "    if exclusion_count>=1:\n",
    "        exclusion_flag=True\n",
    "    return(exclusion_flag,exclusion_count,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_extract(text_list,language):\n",
    "    cond_flag=False\n",
    "    if language=='english':\n",
    "        cond_lis=['condition']\n",
    "    else:\n",
    "        cond_lis=['condición','condiciones','condicionado']\n",
    "   \n",
    "    cond_count=0\n",
    "    position_list=[]\n",
    "    position=0\n",
    "    for text in text_list:\n",
    "        # if any two of 'exclusion ,condition or extension is present in the text, that text is omitted from the logic'\n",
    "        bool_list=['condition' in text.lower(),'exclusion' in text.lower(),'extension' in text.lower()]\n",
    "        if sum(bool_list)>=2:\n",
    "            continue\n",
    "        for i in cond_lis:\n",
    "            for match in re.finditer(i, text.lower()):\n",
    "                position_list.append(position+match.start())               \n",
    "                cond_count+=1\n",
    "        position+=len(text)\n",
    "    if cond_count>=1:\n",
    "        cond_flag=True\n",
    "    return(cond_flag,cond_count,position_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_extraction(soup):\n",
    "    fontsizes=[]\n",
    "    for divs in soup.findAll('div'):\n",
    "        for j in divs.find_all('span'):\n",
    "            ext_size=re.findall(r'font-size:(.*)px\">',str(j))\n",
    "            if ext_size:\n",
    "                fontsizes.append(int(ext_size[0]))\n",
    "    return(fontsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_check(word,text,language):\n",
    "    if language=='spanish':\n",
    "        stop_words = list(set(stopwords.words('spanish')))\n",
    "    else:\n",
    "        stop_words = list(set(stopwords.words('english')))\n",
    "        transition=\"although  instead  whereas  despite  conversely  otherwise  however moreover  likewise  comparatively  correspondingly  similarly  furthermore  additionallyver  rather  nevertheless  nonetheless  regardless  notwithstanding consequently  therefore  thereupon  forthwith  accordingly  henceforth\"\n",
    "        transition_words=transition.split()\n",
    "        transition_words\n",
    "        stop_words.extend(transition_words)\n",
    "\n",
    "    if exclusion_check(text,language):\n",
    "        return True\n",
    "    if word.lower() in stop_words:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclusion_check(text,language):\n",
    "    if language=='spanish':\n",
    "        exclusion_bag=['excepto','excepción','no in cluido','exclusión','excluidos','excluirlo','excluyendo',' excluyen','exclusiones','exclusiones','exclusion','excluyentes']\n",
    "    else:\n",
    "        exclusion_bag=['exclusion','excluded','not covered','will not cover','will not pay']\n",
    "    for word in exclusion_bag:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria_check(text,language):\n",
    "    if language=='spanish':\n",
    "        exclusion_bag=['condición','condiciones','condicionado']\n",
    "    else:\n",
    "        exclusion_bag=['condition','conditions']\n",
    "    for word in exclusion_bag:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_check(text,language):\n",
    "    if language=='english':\n",
    "        ext_bag=['extension']\n",
    "    else:\n",
    "        ext_bag=['extensión','extensiones']\n",
    "    for word in ext_bag:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=re.compile('.*\\d{1,3}.*(?:\\-|\\.|–)')\n",
    "def clean(text):\n",
    "    text=text.replace('\\n','').replace('\\t',' ')\n",
    "    text=start.sub('',text).strip()\n",
    "    \n",
    "\n",
    "    return(text)\n",
    "def clean_pharses(phrases):\n",
    "    phrases=[clean(i) for i in phrases if len(i)> 3]\n",
    "    phrases=list(set(phrases))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_index(path,subdir,language,country):\n",
    "    pointsRE_heading=re.compile('(?:\\s*\\([a-z]{1,3}\\)|(?<![A-z])[A-Z]{1}\\s+[a-zA-Z0-9_\\s]{5})')\n",
    "    pageNumRE=re.compile('Page\\s*(\\d{1,3})',re.IGNORECASE)\n",
    "    neglect_def=['policy','insured','schedule']\n",
    "    local_indexed={}\n",
    "    previous_span=''\n",
    "    bold_phrases_indexed={}\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.pdf'):\n",
    "            print(file)\n",
    "            global file_count\n",
    "            file_count+=1\n",
    "            \n",
    "            print(\"COUNT\",file_count)\n",
    "            try:\n",
    "                pg_no=len(list(extract_pages(os.path.join(path,file))))\n",
    "            except:\n",
    "                pg_no=0\n",
    "            try:\n",
    "                html=convert_pdf(os.path.join(path,file),'html')\n",
    "            except:\n",
    "                continue\n",
    "            soup = BeautifulSoup(html, 'html5lib')\n",
    "            underline_positions=get_underlines(soup)\n",
    "            try:\n",
    "                fontsizes=font_extraction(soup)\n",
    "                file_font_size_mode=mode(fontsizes)\n",
    "            except:\n",
    "                file_font_size_mode=8\n",
    "                \n",
    "            text_para,text_plain_para='',''\n",
    "            text_lis,text_plain_lis,bold_lis,page_list,=[],[],[],[]\n",
    "            bold=False\n",
    "            sub_page_def_list=[]\n",
    "            try:\n",
    "                definitions=def_extraction(os.path.join(path,file))\n",
    "            except:\n",
    "                continue\n",
    "            definitions={key:value for key,value in definitions.items() if key.lower().strip() not in neglect_def}\n",
    "            def_terms=list(definitions.keys())\n",
    "#             print(len(def_terms),'LEN DEFINITIONS')\n",
    "#             print(definitions)\n",
    "            definition_text=''\n",
    "            header_match_object=(0,'',False)\n",
    "            cond_header_match_object=(0,'',False)\n",
    "            ext_header_match_object=(0,'',False)\n",
    "            second_category=False\n",
    "            head_found=False\n",
    "            def_flag=False\n",
    "            cond_head_found=False\n",
    "            ext_head_found=False\n",
    "            single_page_head_found=False\n",
    "            condition_text=''\n",
    "            ext_text=''\n",
    "            excl_text=''\n",
    "            previous_pg_num=[]\n",
    "            def_in_page=[]\n",
    "            cond_single_page_head_found=False\n",
    "            ext_single_page_head_found=False\n",
    "            \n",
    "            font_size=file_font_size_mode\n",
    "\n",
    "            for divs in soup.findAll('div'):\n",
    "                div_text_list=[span.text for span in divs.find_all('span') ]\n",
    "                page_str=str(divs.find_all('a'))\n",
    "                page_num_results=pageNumRE.findall(page_str)\n",
    "                if page_num_results:\n",
    "                    pagenum=page_num_results[0]\n",
    "#                     print(pagenum)\n",
    "                    if pagenum!=previous_pg_num:\n",
    "                        sub_definitions_in_page=[]\n",
    "                    previous_pg_num=pagenum\n",
    "#                     definition_text=''\n",
    "                    \n",
    "                    #only for pdfs\n",
    "                    if(int(pagenum) in [1,2]) and 'Wording' in file and pg_no >20:\n",
    "#                     print('Continued on wording 2')\n",
    "                        continue\n",
    "\n",
    "                    if text_lis:\n",
    "                        text_lis=text_lis[:-1]\n",
    "                        text_para=''.join(text_lis)\n",
    "                        text_plain_lis=text_plain_lis[:-1]\n",
    "                        text_plain_para=''.join(text_plain_lis)\n",
    "                        if(len(re.findall('\\.',text_para))>(len(text_para)/3) or (len(re.findall('\\d',text_para))>80 and int(pagenum)<4 and  pg_no>20 and 'content' in text_para.lower())) :\n",
    "                            print('PASS')\n",
    "                            print(text_plain_para)\n",
    "                            text_para,text_plain_para=' ',' '\n",
    "                            text_lis,text_plain_lis=[],[]\n",
    "                            single_page_head_found=False\n",
    "                            head_found=False\n",
    "                            cond_head_found=False\n",
    "                            ext_head_found=False\n",
    "                            continue\n",
    "                        \n",
    "                        else:\n",
    "                            bold_lis=clean_pharses(bold_lis)\n",
    "                            pagenum=int(pagenum)-1\n",
    "                            excl_flag,excl_count,excl_pos_lis=excl_extract(text_plain_lis,language)\n",
    "                            cond_flag,cond_count,cond_pos_lis=cond_extract(text_plain_lis,language)\n",
    "                            ext_flag,ext_count,ext_pos_lis=ext_extract(text_plain_lis,language)\n",
    "                            if int(pagenum)==1:\n",
    "                                if head_found or single_page_head_found or excl_count>0:\n",
    "#                                     print(pagenum,True,'!!!')\n",
    "                                    excl_flag=True\n",
    "                                else:\n",
    "                                    excl_flag=False\n",
    "                            else:\n",
    "                                if head_found or single_page_head_found:\n",
    "#                                     print(pagenum,True,'!!!',excl_count)\n",
    "                                    excl_flag=True\n",
    "#                                     print(excl_flag,'EXCL')\n",
    "#                                 else:\n",
    "#                                     print(pagenum,excl_flag,'\\n')\n",
    "# #                             print(excl_text)\n",
    "#                             print(condition_text)\n",
    "                            if cond_head_found or cond_single_page_head_found or cond_count>0:\n",
    "                                cond_flag=True\n",
    "#                                 print('COND','TRUE')\n",
    "                            else:\n",
    "#                                 print('COND','FALSE')\n",
    "                                cond_flag=False\n",
    "    \n",
    "                            if ext_head_found or ext_single_page_head_found or ext_count>0:\n",
    "                                ext_flag=True\n",
    "#                                 print('EXT','TRUE',pagenum)\n",
    "                            else:\n",
    "#                                 print('EXT','FALSE',pagenum)\n",
    "                                ext_flag=False\n",
    "#                             def_in_page=[{'name':k,'text':v[0] for k,v in definitions.items() if v[1]=pagenum}]\n",
    "                            for term,defs in definitions.items():\n",
    "#                                 print(defs[1])\n",
    "                                if defs[1]==pagenum:  ###defs[1] is the pagenumber\n",
    "                                    definition_text=definition_text+'          '+term+' '+defs[0]\n",
    "                                    def_in_page.append({'name':term,'text':defs[0]})\n",
    "#                                     def_in_page{'name':}\n",
    "#                             print(definition_text)\n",
    "                            if definition_text:\n",
    "                                def_flag=True\n",
    "                            \n",
    "                            if def_flag:\n",
    "                                def_search_flag=True\n",
    "#                                 print('DEF','TRUE')\n",
    "                            else:\n",
    "#                                 print('DEF', 'FALSE')\n",
    "                                def_search_flag=False   \n",
    "                            \n",
    "                            if pg_no>2:\n",
    "                                endorsements=False\n",
    "                            else:\n",
    "                                endorsements=True\n",
    "#                             print(endorsements,'ENDORSEMENTS!!!')\n",
    "                            if text_para in local_indexed.keys():\n",
    "                                text_para=text_para+' '\n",
    "                            \n",
    "                            local_indexed[text_para]=(file,pagenum,bold_lis,text_plain_para,excl_flag,excl_count,excl_pos_lis,subdir,sub_page_def_list,country,language,definition_text,def_search_flag,endorsements,excl_text,condition_text,cond_flag,cond_count,cond_pos_lis,def_in_page,ext_text,ext_flag,ext_count,ext_pos_lis)\n",
    "#                             print(sub_page_def_list,'DEF LIST!!!!!1')\n",
    "                            sub_page_def_list=[]\n",
    "                            page_list.append(pagenum)\n",
    "                            text_para,text_plain_para='',''\n",
    "                            text_lis,text_plain_lis,bold_lis=[],[],[]\n",
    "                            definition_text=''\n",
    "                            cond_single_page_head_found=False\n",
    "                            ext_single_page_head_found=False\n",
    "                            condition_text=''\n",
    "                            ext_text=''\n",
    "                            excl_text=''\n",
    "                            single_page_head_found=False\n",
    "                            def_in_page=[]\n",
    "                            def_flag=False\n",
    "                for span in divs.find_all('span'):\n",
    "\n",
    "                    bold=False\n",
    "                    upper=False\n",
    "                    bullet=False\n",
    "                    def_flag=False\n",
    "                    span_position=div_text_list.index(span.text)\n",
    "                    \n",
    "                    if \"Bold\" in str(span) or 'CIDFont+F3' in str(span):\n",
    "                        bold_lis.append(span.text)\n",
    "                        bold=True\n",
    "                    if span.text.isupper():\n",
    "                        upper=True\n",
    "                    font_family_match=re.findall(r\"font-family: b'(.*)';\",str(span))\n",
    "                    if font_family_match:\n",
    "                        font_family=font_family_match[0]\n",
    "                    else:\n",
    "                        font_family=''\n",
    "#                     print(font_family)\n",
    "                    font_size_match=re.findall(r'font-size:(.*)px\">',str(span))\n",
    "                    if font_size_match:\n",
    "                        font_size=int(font_size_match[0])\n",
    "                    if pointsRE_heading.findall(span.text):\n",
    "                        bullet=True\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    y_cord_list=y_cordinate_Re.findall(str(divs))\n",
    "                    if y_cord_list:\n",
    "                        y_cord=int(y_cord_list[0])\n",
    "                    underlined_text=check_underline(underline_positions,y_cord)\n",
    "                    if span.text.split('\\n')[0]!='' and span.text.split('\\n')[0]!=' ':\n",
    "                        head_check_text=span.text.split('\\n')[0]\n",
    "                    elif len(span.text.split('\\n'))>1:\n",
    "                        head_check_text=span.text.split('\\n')[1]\n",
    "                    else:\n",
    "                        head_check_text=''\n",
    "##############################3  \n",
    "                    if str(previous_span).endswith('<br/></span>'):\n",
    "                        span_position=0\n",
    "                    ##################INSERT FUNCTION####################z\n",
    "                    #EXCLUSION \n",
    "                    if  span_position==0 and (bold or font_size>= file_font_size_mode +2 ) and (font_size> file_font_size_mode or upper) and 5<len(head_check_text.strip())<80:                    \n",
    "                        if exclusion_check(head_check_text,language) and not head_found: \n",
    "                            head_found=True\n",
    "                            header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "                            First_category=True\n",
    "                            second_category=False\n",
    "                            single_page_head_found=True\n",
    "#                             print('FOUND Heading for exclusion ...',span.text)\n",
    "                        \n",
    "                    \n",
    "                        elif ((font_size,font_family,bold,upper,bullet)==header_match_object  or font_size>header_match_object[0] ) and head_found and not exclusion_check(head_check_text,language):\n",
    "                            head_found=False\n",
    "                            single_page_head_found=True\n",
    "#                             print('FOUND Closure for exclusion ...',span.text)\n",
    "#                     if 'CLAIMS CONDITIONS' in span.text:\n",
    "#                         print ('^^^^^^^',(font_size,font_family,bold,upper,bullet),header_match_object, font_size>header_match_object[0] , head_found , not exclusion_check(head_check_text,language))\n",
    "#                         print(pointsRE_heading.findall(span.text))\n",
    "                    words_title=[word.istitle() for word in head_check_text.split() if stopword_check(word,head_check_text,language) and not word.isdigit() ]\n",
    "                    if span_position==0 and all(words_title) and head_check_text.strip() not in definitions and  (len(words_title) >1 or underlined_text )and 5<len(head_check_text.strip())<80 :\n",
    "#                         print('MMMMMMMMARARARA',span.text)\n",
    "\n",
    "                        if exclusion_check(head_check_text,language) and  not head_found:\n",
    "                            head_found=True\n",
    "                            second_category=True\n",
    "                            First_category=False\n",
    "                            single_page_head_found=True\n",
    "                            header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "#                             print(\"FOUND heading type2 ....\",span.text)\n",
    "                        elif head_found and not exclusion_check(head_check_text,language) and second_category and ((font_size,font_family,bold,upper,bullet)==header_match_object or font_size > header_match_object[0] ) :\n",
    "                            head_found=False\n",
    "                            second_category=False\n",
    "                            single_page_head_found=True\n",
    "#                             print('FOUND closure type 2',span.text)\n",
    "                    if head_found:\n",
    "                        excl_text=excl_text+' '+span.text     \n",
    "                     \n",
    "                    #CONDITIONS\n",
    "#                     if 'Condition' in span.text:\n",
    "# #                         print('text:',str(span.text),'head check',head_check_text,'length of head check',len(head_check_text))\n",
    "# #                         print(span_position==0,(bold or font_size>= file_font_size_mode +2 ), (font_size> file_font_size_mode or upper), 5<len(head_check_text.strip())<80)\n",
    "# #                         print(font_size,file_font_size_mode,cond_head_found,criteria_check(head_check_text,language))\n",
    "                    if  span_position==0 and (bold or font_size>= file_font_size_mode +2 ) and (font_size> file_font_size_mode or upper) and 5<len(head_check_text.strip())<80:                    \n",
    "                        if criteria_check(head_check_text,language) and not cond_head_found: \n",
    "                            cond_head_found=True\n",
    "                            cond_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "                            cond_First_category=True\n",
    "                            cond_second_category=False\n",
    "                            cond_single_page_head_found=True\n",
    "#                             print('FOUND Heading for condition...',span.text)\n",
    "                    \n",
    "                        elif ((font_size,font_family,bold,upper,bullet)==cond_header_match_object or font_size>cond_header_match_object[0] ) and not criteria_check(head_check_text,language)  and cond_head_found:\n",
    "                            cond_head_found=False\n",
    "                            cond_single_page_head_found=True\n",
    "#                             print('FOUND Closure for condition ...',span.text)\n",
    "                \n",
    "                    \n",
    "                    cond_words_title=[word.istitle() for word in head_check_text.split() if stopword_check(word,head_check_text,language) and not word.isdigit() ]\n",
    "                    if span_position==0 and all(cond_words_title) and head_check_text.strip() not in definitions and  (len(words_title) >1 or underlined_text )and 5<len(head_check_text.strip())<80 :\n",
    "#                         print('MMMMMMMMARARARA',span.text)\n",
    "\n",
    "                        if criteria_check(head_check_text,language) and  not cond_head_found:\n",
    "                            cond_head_found=True\n",
    "                            cond_second_category=True\n",
    "                            cond_First_category=False\n",
    "                            cond_single_page_head_found=True\n",
    "                            cond_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "#                             print(\"FOUND heading type2 ....\",span.text)\n",
    "                        elif cond_head_found and not criteria_check(head_check_text,language) and cond_second_category and ((font_size,font_family,bold,upper,bullet)==cond_header_match_object or font_size > cond_header_match_object[0] ) :\n",
    "                            cond_head_found=False\n",
    "                            cond_second_category=False\n",
    "                            cond_single_page_head_found=True\n",
    "#                             print('FOUND closure type 2',span.text)\n",
    "\n",
    "                    if cond_head_found==True:\n",
    "                        condition_text=condition_text+' '+span.text\n",
    "                            \n",
    "                    \n",
    "                    #####EXTENSIONS\n",
    "                    #CONDITIONS\n",
    "#                     if 'Extension' in span.text:\n",
    "#                         print(span.text)\n",
    "#                         print('text:',str(span.text),'head check',head_check_text,'length of head check',len(head_check_text))\n",
    "#                         print(span_position==0,(bold or font_size>= file_font_size_mode +2 ), (font_size> file_font_size_mode or upper), 5<len(head_check_text.strip())<80)\n",
    "#                         print(font_size,file_font_size_mode,cond_head_found,criteria_check(head_check_text,language))\n",
    "                    if  span_position==0 and (bold or font_size>= file_font_size_mode +2 ) and (font_size> file_font_size_mode or upper) and 5<len(head_check_text.strip())<80:                    \n",
    "                        if ext_check(head_check_text,language) and not ext_head_found: \n",
    "                            ext_head_found=True\n",
    "                            ext_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "                            ext_First_category=True\n",
    "                            ext_second_category=False\n",
    "                            ext_single_page_head_found=True\n",
    "#                             print('FOUND Heading for ext...',span.text)\n",
    "                    \n",
    "                        elif ((font_size,font_family,bold,upper,bullet)==ext_header_match_object or font_size>ext_header_match_object[0] ) and not ext_check(head_check_text,language)  and ext_head_found:\n",
    "                            ext_head_found=False\n",
    "                            ext_single_page_head_found=True\n",
    "#                             print('FOUND Closure for ext ...',span.text)\n",
    "                \n",
    "                    \n",
    "                    ext_words_title=[word.istitle() for word in head_check_text.split() if stopword_check(word,head_check_text,language) and not word.isdigit() ]\n",
    "                    if span_position==0 and all(ext_words_title) and head_check_text.strip() not in definitions and  (len(words_title) >1 or underlined_text )and 5<len(head_check_text.strip())<80 :\n",
    "#                         print('MMMMMMMMARARARA',span.text)\n",
    "\n",
    "                        if ext_check(head_check_text,language) and  not ext_head_found:\n",
    "                            ext_head_found=True\n",
    "                            ext_second_category=True\n",
    "                            ext_First_category=False\n",
    "                            ext_single_page_head_found=True\n",
    "                            ext_header_match_object=(font_size,font_family,bold,upper,bullet)\n",
    "#                             print(\"FOUND heading type2 ext....\",span.text)\n",
    "                        elif ext_head_found and not ext_check(head_check_text,language) and ext_second_category and ((font_size,font_family,bold,upper,bullet)==ext_header_match_object or font_size > ext_header_match_object[0] ) :\n",
    "                            ext_head_found=False\n",
    "                            ext_second_category=False\n",
    "                            ext_single_page_head_found=True\n",
    "#                             print('FOUND closure type 2 ext',span.text)\n",
    "# \n",
    "                    if ext_head_found==True:\n",
    "                        ext_text=ext_text+' '+span.text\n",
    "                            \n",
    "    #                     else:\n",
    "###############################             \n",
    "                        \n",
    "                    if font_size_match:\n",
    "                        if font_size<=file_font_size_mode-1:\n",
    "                            text_lis.append(' ')\n",
    "                    text=span.text.replace('\\n',' ')\n",
    "                    text_plain=text.replace('\\n',' ')\n",
    "                    for term in def_terms:\n",
    "                        if term.lower().strip() in text.lower() and definitions[term][0].strip() not in text and term not in sub_definitions_in_page:\n",
    "#                             def_page.append(definitions[term])\n",
    "#                             print(pagenum)\n",
    "                            src_str  = re.compile(re.escape(term), re.IGNORECASE)\n",
    "                            text=src_str.sub('###{}@@{}%%%'.format(term,definitions[term][0]),text)\n",
    "#                             print(text,'\\n',term,'TERM!!!!@#$%^&*&^%#$%^&')\n",
    "                            sub_definitions_in_page.append(term)\n",
    "                            sub_page_def_list.append({'name':term,'text':definitions[term][0]})\n",
    "#                             print(definitions_in_page)\n",
    "                            break\n",
    "#                     print(definitions_in_page)\n",
    "\n",
    "                    \n",
    "                    text_lis.append(text)\n",
    "                    text_plain_lis.append(text_plain)\n",
    "                    previous_span=span\n",
    "                    \n",
    "\n",
    "            if text_lis:\n",
    "                if(int(pagenum) in [1,2]) and 'Wording' in file and pg_no >20:\n",
    "#                     print('Continued on wording 2')\n",
    "                    continue\n",
    "\n",
    "                text_lis=text_lis[:-1]\n",
    "                text_para=''.join(text_lis)\n",
    "                text_plain_lis=text_plain_lis[:-1]\n",
    "                text_plain_para=''.join(text_plain_lis)\n",
    "                if(len(re.findall('\\.',text_para))>(len(text_para)/3) or (len(re.findall('\\d',text_para))>80 and int(pagenum)<4 and pg_no>20 and 'content' in text_para.lower())) :\n",
    "                    print('PASS')\n",
    "                    print(text_plain_para)\n",
    "                    text_para,text_plain_para=' ',' '\n",
    "                    text_lis,text_plain_lis=[],[]\n",
    "                    single_page_head_found=False\n",
    "                    head_found=False\n",
    "                    cond_head_found=False\n",
    "                    ext_head_found=False\n",
    "                    continue\n",
    "                else:\n",
    "                    bold_lis=clean_pharses(bold_lis)\n",
    "                    if pagenum in page_list and pagenum!='1':\n",
    "                        pagenum=int(pagenum)+1\n",
    "                    excl_flag,excl_count,excl_pos_lis=excl_extract(text_plain_lis,language)\n",
    "                    cond_flag,cond_count,cond_pos_lis=cond_extract(text_plain_lis,language)\n",
    "                    ext_flag,ext_count,ext_pos_lis=ext_extract(text_plain_lis,language)\n",
    "                    if int(pagenum)==1:\n",
    "                        if head_found or single_page_head_found or excl_count>0:\n",
    "#                             print(pagenum,True,'!!!',excl_count)\n",
    "                            excl_flag=True\n",
    "                        else:\n",
    "                            excl_flag=False\n",
    "                    else:\n",
    "                        if head_found or single_page_head_found:\n",
    "#                             print(pagenum,True,'!!!')\n",
    "                            excl_flag=True\n",
    "#                             print(excl_flag,'EXCL')\n",
    "                        \n",
    "                    #####################\n",
    "                    if cond_head_found or cond_single_page_head_found or cond_count>0:\n",
    "                        cond_flag=True\n",
    "#                         print('COND','TRUE')\n",
    "                    else:\n",
    "                        cond_flag=False\n",
    "                    \n",
    "                    if ext_head_found or ext_single_page_head_found or ext_count>0:\n",
    "                        ext_flag=True\n",
    "#                         print('EXT','TRUE',pagenum)\n",
    "                    else:\n",
    "#                         print('EXT','FALSE',pagenum)\n",
    "                        ext_flag=False\n",
    "#                         print('COND','FALSE')\n",
    "#                     page_def_list.append({'name':term,'text':definitions[term][0]}\n",
    "#                     def_in_page=[{'name':k,'text':v[0]} for k,v in definitions.items() if v[1]==pagenum]  \n",
    "#                     print(def_in_page,'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                    for term,defs in definitions.items():\n",
    "                        if defs[1]==pagenum:  ###defs[1] is the pagenumber\n",
    "                            definition_text=definition_text+'          '+term+' '+defs[0]\n",
    "                            def_in_page.append({'name':term,'text':defs[0]})        \n",
    "#                     print(definition_text)\n",
    "                    if definition_text:\n",
    "                        def_flag=True\n",
    "\n",
    "                    if def_flag:\n",
    "                        def_search_flag=True\n",
    "#                         print('DEF','TRUE')\n",
    "                    else:\n",
    "                        def_search_flag=False\n",
    "#                         print('DEF','FALSE')\n",
    "                    \n",
    "                    if pg_no>2:\n",
    "                        endorsements=False\n",
    "                    else:\n",
    "                        endorsements=True\n",
    "#                     print(endorsements,'ENDORSEMENTS!!!')        \n",
    "                    if text_para in local_indexed.keys():\n",
    "                        text_para=text_para+' '\n",
    "                    local_indexed[text_para]=(file,pagenum,bold_lis,text_plain_para,excl_flag,excl_count,excl_pos_lis,subdir,sub_page_def_list,country,language,definition_text,def_search_flag,endorsements,excl_text,condition_text,cond_flag,cond_count,cond_pos_lis,def_in_page,ext_text,ext_flag,ext_count,ext_pos_lis)\n",
    "#                     print(sub_page_def_list,'DEF LIST!!!!!1')\n",
    "                    sub_page_def_list=[]\n",
    "                    text_para,text_plain_para='','' \n",
    "                    text_plain_lis,text_lis,bold_lis=[],[],[]\n",
    "                    definition_text=''\n",
    "                    single_page_head_found=False\n",
    "                    cond_single_page_head_found=False\n",
    "                    ext_single_page_head_found=False\n",
    "                    condition_text=''\n",
    "                    ext_text=''\n",
    "                    excl_text=''\n",
    "                    def_in_page=[]\n",
    "                    def_flag=False\n",
    "    return(local_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_pdf_path(path):\n",
    "    file_list=os.listdir(path)\n",
    "    for file in file_list:\n",
    "        raw_filename=file[:file.rfind(\".\")]\n",
    "        pdf_filename=raw_filename+'.pdf'\n",
    "#         print(raw_filename,pdf_filename,'FILE!!')\n",
    "        if pdf_filename in file_list:\n",
    "            print('continued!!!!!!!!!!!!!')\n",
    "            continue\n",
    "        if file.endswith('doc') or file.endswith('docx'):\n",
    "            print('processing!!!')\n",
    "            wordToPdf(os.path.join(path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootDir='V://COG//AU- Property//'\n",
    "# for dirname,subdirlist,filelist in os.walk(rootDir):\n",
    "#     print(dirname)\n",
    "#     for subdir in subdirlist:\n",
    "#         path=os.path.join(dirname,subdir)\n",
    "# #         doc_pdf_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chile Cas Pw 01 SUBDIR!!!!!!!!!!!!!\n",
      "CP MATIC KARD - SALCOBRAND AD - 12-7639381.pdf\n",
      "COUNT 1\n",
      "CP MATIC KARD - PREUNIC  AD PLUS _Asistencia - 12-7643970.pdf\n",
      "COUNT 2\n",
      "CP MATIC KARD - SALCOBRAND  HC - 12-7640444.pdf\n",
      "COUNT 3\n",
      "CP Falabella - HC+ASIST (VACAM6).pdf\n",
      "COUNT 4\n",
      "CP AUTOCLUB CHILE - HC.pdf\n",
      "COUNT 5\n",
      "CP PREUNIC  AD UP SELL.pdf\n",
      "COUNT 6\n",
      "CP Falabella - AD+ITP (VACAP5).pdf\n",
      "COUNT 7\n",
      "CP PREUNIC - HC Plan UP SELL.pdf\n",
      "COUNT 8\n",
      "CP Matic Kard - HC - 12-7639384.pdf\n",
      "COUNT 9\n",
      "CP PREUNIC  - HC Plan PLUS _ ASISTENCIA 12-7643737.pdf\n",
      "COUNT 10\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "file_count=0\n",
    "rootDir='Documents/CH-Casualty/'\n",
    "country='chile'\n",
    "local_indexed={}\n",
    "language='spanish'\n",
    "if 'spanish' in rootDir:\n",
    "    language='spanish'\n",
    "for dirname,subdirlist,filelist in os.walk(rootDir):\n",
    "    for subdir in subdirlist:\n",
    "        print(subdir, 'SUBDIR!!!!!!!!!!!!!')\n",
    "        path=os.path.join(dirname,subdir)    \n",
    "        local_indexed_subdir=create_local_index(path,subdir,language,country)\n",
    "        local_indexed.update(local_indexed_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(local_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "\n",
    "cosmo_endpoint=\"https://nf-poc-cdb-sql.documents.azure.com\"\n",
    "cosmo_key=\"iEcEfrxYe0Fm9QtoxDrOpLvGsfzjowwybULlWT9Uz4XxV4RmOIAnRuLdgRFUu1LPU5Vwk3UGivRrPrxnk7083w==\"\n",
    "client = CosmosClient(cosmo_endpoint, cosmo_key)\n",
    "database=client.get_database_client('policy-analysis')\n",
    "container=database.get_container_client('casualty-wordings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in local_indexed.items():\n",
    "    doc={}\n",
    "    doc['text']=key\n",
    "    doc['page']=int(value[1])\n",
    "    if value[4]==True or value[0].find('Exclusion')!=-1:\n",
    "        doc['IsExclusion']=True\n",
    "    else:\n",
    "        doc['IsExclusion']=False\n",
    "    doc['doc_name']=value[0].replace('.pdf','')\n",
    "    doc['id']=doc['doc_name']+' '+str(doc['page'])\n",
    "    doc['bold_phrases']=value[2]\n",
    "    doc['plain_text']=value[3]\n",
    "    doc['excl_count']=value[5]\n",
    "    doc['excl_pos']=value[6]\n",
    "    doc['folder']=value[7]\n",
    "    doc['definitions']=[{'name':key,'text':value} for key,value in re.findall('###(.*?)@@(.*?)%%%',doc['text'])]\n",
    "    doc['country']=value[9]\n",
    "    doc['language']=value[10]\n",
    "    doc['definition_text']=value[11]\n",
    "#     print(doc['definition_text'],'\\n\\n')\n",
    "    if doc['definition_text']:\n",
    "        doc['definition_flag']=True\n",
    "    else:\n",
    "        doc['definition_flag']=False\n",
    "    if value[14]==True or 'endorsement' in value[0].lower(): \n",
    "        doc['endorsements']=True\n",
    "    else:\n",
    "        doc['endorsements']=value[13]\n",
    "    doc['excl_text']=value[14]\n",
    "    doc['cond_text']=value[15]\n",
    "    doc['cond_flag']=value[16]\n",
    "    doc['cond_count']=value[17]\n",
    "    doc['cond_pos']=value[18]\n",
    "    doc['definitions_in_page']=value[19]\n",
    "    doc['ext_text']=value[20]\n",
    "    doc['ext_flag']=value[21]\n",
    "    doc['ext_count']=value[22]\n",
    "    doc['ext_pos']=value[23]\n",
    "    \n",
    "    try:       \n",
    "        container.create_item(body=doc)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added to cosmos\n"
     ]
    }
   ],
   "source": [
    "print('added to cosmos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
