{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "def search_relevance (score):\n",
    "        match,relevance = \"no\",\"\"\n",
    "        if score>=99:\n",
    "            match,relevance = \"full\",\"\"\n",
    "        elif score >=90:\n",
    "            match,relevance = \"partial\",\"high\"\n",
    "        elif score >=70:\n",
    "            match,relevance = \"partial\",\"medium\"\n",
    "        elif 0<score <70:\n",
    "            match,relevance = \"partial\",\"low\"\n",
    "        return match,relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import fitz\n",
    "\n",
    "from pdf2txt import convert_pdf\n",
    "from bs4 import BeautifulSoup,NavigableString\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchTypeRelevance(a) : \n",
    "#     print(a)\n",
    "    matchtype_agg=\"\"\n",
    "    relevance_agg=\"\"\n",
    "    for i in a:\n",
    "        matchtype = i.get(\"matchType\",\"\")\n",
    "        relevance = i.get(\"relevance\",\"\")\n",
    "        \n",
    "        \n",
    "        if matchtype==\"partial\" :\n",
    "            \n",
    "            if matchtype_agg not in(\"no\"):\n",
    "                matchtype_agg=matchtype\n",
    "            elif matchtype_agg==\"\":\n",
    "                matchtype_agg=matchtype\n",
    "                \n",
    "            \n",
    "                \n",
    "        elif matchtype==\"full\" :\n",
    "            if matchtype_agg not in(\"partial\",\"no\"):\n",
    "                matchtype_agg=matchtype\n",
    "            elif matchtype_agg==\"\":\n",
    "                matchtype_agg=matchtype\n",
    "        elif matchtype==\"no\":\n",
    "            matchtype_agg=matchtype\n",
    "        \n",
    "        if relevance==\"low\" :\n",
    "            relevance_agg=relevance\n",
    "        elif relevance==\"medium\" :\n",
    "            \n",
    "            if relevance_agg not in(\"low\"):\n",
    "                relevance_agg=relevance\n",
    "            elif relevance_agg==\"\":\n",
    "                relevance_agg=relevance\n",
    "            \n",
    "        elif relevance==\"high\" :\n",
    "            \n",
    "            if relevance_agg not in(\"medium\",\"low\"):\n",
    "                relevance_agg=relevance\n",
    "            elif relevance_agg==\"\":\n",
    "                relevance_agg=relevance\n",
    "            \n",
    "    if matchtype_agg==\"no\" or matchtype_agg==\"full\":\n",
    "        relevance_agg=\"\"\n",
    "   \n",
    "    return matchtype_agg,relevance_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabula import read_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(doc_name,pages,doc_path=r\"../../../Trace_files/PDBI/\"):\n",
    "    paragraph=\"\"\n",
    "    doc=fitz.open(os.path.join(doc_path,doc_name))\n",
    "    for page_no in pages:\n",
    "\n",
    "        page = doc.loadPage(page_no-1)\n",
    "        paragraph=paragraph+page.getText(\"html\")+\"\\n\"\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_html(doc_name,pages,doc_path=r\"../../../Trace_files/PDBI\"):\n",
    "    fitz_doc = fitz.open(os.path.join(doc_path,doc_name))\n",
    "    html_full=''\n",
    "    wording_ref=[]\n",
    "    footer_dict={}\n",
    "    total_pages=len(fitz_doc)\n",
    "#     print(total_pages)\n",
    "    for page_no in pages:\n",
    "        page = fitz_doc.loadPage(page_no-1)\n",
    "        \n",
    "\n",
    "        page_html=page.getText(\"html\")\n",
    "       \n",
    "        page_html=page_html.replace('position','page:{};position'.format(page_no))\n",
    "        soup = BeautifulSoup(page_html, 'html5lib')\n",
    "        toc_page_no=[]\n",
    "        dot_count=0\n",
    "        introflag=False\n",
    "\n",
    "        for p in soup.findAll('p'):        \n",
    "            p_str=str(p)\n",
    "            dot_count+=p_str.count('.')\n",
    "\n",
    "            try:        \n",
    "                xcord=int(x_cordinate_Re.findall(p_str)[0])\n",
    "            except:\n",
    "                xcord=0 \n",
    "            try:\n",
    "                ycord=int(y_cordinate_Re.findall(p_str)[0])\n",
    "            except:\n",
    "                ycord=0\n",
    "\n",
    "            for span in p.findAll('span'):\n",
    "                text=span.text.strip()\n",
    "                try:\n",
    "                    toc_page=int(span.text)\n",
    "\n",
    "\n",
    "                    if page_no< total_pages and xcord>500:\n",
    "                        toc_page_no.append(toc_page)                   \n",
    "                except Exception as err:\n",
    "                    pass\n",
    "                try:\n",
    "                    font_size=float(font_size_RE.search(p_str).group(1))\n",
    "                except Exception as err:\n",
    "                    print(err)\n",
    "                    font_size=8\n",
    "\n",
    "                if xcord < 150 and ycord >720 and font_size <10 :\n",
    "\n",
    "\n",
    "                    wording_ref.append(text)\n",
    "                if  ycord >720  and font_size <10 :\n",
    "                    footer_dict[text]=ycord\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "        if len(toc_page_no) >10 or dot_count>1500 :\n",
    "#             print('table of content',page_no)\n",
    "            continue\n",
    "        if 'img' in page_html:\n",
    "#             print('Image',page_no)\n",
    "            continue\n",
    "        if 'Introduction' in page_html and 'www.chubb.com' in page_html:\n",
    "#             print('Intro',page_no)\n",
    "            continue\n",
    "\n",
    "#         if page_no <=16:\n",
    "#             df=read_pdf(file,  pages=page_no)\n",
    "#             if df:\n",
    "#                 print('TABLE FOUND IN ',page_no)\n",
    "#                 continue\n",
    "\n",
    "\n",
    "        html_full+=page_html\n",
    "\n",
    "    wording_ref=[i for i in wording_ref if len(i)>2]\n",
    "    wording_reference_p_text=max(set(wording_ref), key = wording_ref.count)\n",
    "    footer_y_cord=footer_dict[wording_reference_p_text]-20\n",
    "    wording_reference=[i for i in wording_reference_p_text.split('    ') if len(i)>0][0]\n",
    "    \n",
    "    html_full=html_full.replace('width:-2pt;height:-2pt','width:596pt;height:842pt')\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    soup = BeautifulSoup(html_full, 'html5lib')\n",
    "\n",
    "    for p in soup.findAll('p'):\n",
    "        p_str=str(p)\n",
    "        text=p.text.strip()\n",
    "\n",
    "\n",
    "        try:        \n",
    "            xcord=int(x_cordinate_Re.findall(p_str)[0])\n",
    "        except:\n",
    "            xcord=0 \n",
    "        try:\n",
    "            ycord=int(y_cordinate_Re.findall(p_str)[0])\n",
    "        except:\n",
    "            ycord=0\n",
    "\n",
    "        try:\n",
    "            font_size=float(font_size_RE.search(p_str).group(1))\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            font_size=8\n",
    "        if \"policy number:\" in p.text.lower() and ycord <100:\n",
    "#             print(p.text)\n",
    "            p.string=''\n",
    "        if  font_size >40 :\n",
    "#             print(p.text,'WATERMARKKK')\n",
    "            p.string=''\n",
    "    \n",
    "        if len(p.text)>4:    \n",
    "            if ycord >footer_y_cord and (font_size <8 or (p.text.strip()==wording_reference.strip())):\n",
    "#                 print(p.text,'refff')\n",
    "                p.string=''\n",
    "        if font_size < 7 :\n",
    "#             print(p.text,'less than 7 ')\n",
    "            p.string=''\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    return soup,wording_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_content = str(soup1)\n",
    "# Html_file= open(\"doc_mod.html\",\"w\",encoding='utf-8')\n",
    "# Html_file.write(html_content)\n",
    "# Html_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pointsRE_heading=re.compile('(?:\\s*\\([a-z]{1,3}\\)|[A-Z]{1}\\s+[a-zA-Z0-9_\\s]{5})')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "y_cordinate_Re=re.compile(';top:(\\d{1,5})pt')  #top:766pt\n",
    "x_cordinate_Re=re.compile(';left:(\\d{1,5})pt')\n",
    "height_re=re.compile('height:(\\d{1,5})pt')\n",
    "font_size_RE = re.compile(\"font-size:(.*?)pt\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_match_patch as dmp_module\n",
    "\n",
    "dmp = dmp_module.diff_match_patch()\n",
    "\n",
    "\n",
    "def get_dif(str1,str2):\n",
    "    dmp.Diff_Timeout = 0\n",
    "    \n",
    "#     if 10000 <len(str2)<20000:\n",
    "#          dmp.Diff_Timeout = 20\n",
    "#     elif 20000<=len(str2)<25000:\n",
    "#         dmp.Diff_Timeout = 60\n",
    "#     elif 25000<=len(str2)<30000:\n",
    "#         dmp.Diff_Timeout = 120\n",
    "#     elif 30000<=len(str2)<35000:\n",
    "#         dmp.Diff_Timeout = 180\n",
    "#     elif len(str2)>35000:\n",
    "#         dmp.Diff_Timeout = 240\n",
    "    \n",
    "\n",
    "#     print(        dmp.Diff_Timeout )   \n",
    "    d = dmp.diff_main(str1, str2)\n",
    "    dmp.diff_cleanupSemantic(d)\n",
    "\n",
    "    paragraph=dmp.diff_prettyHtml( d)\n",
    "    return paragraph,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_match(str1,str2):\n",
    "\n",
    "    if str1 in str2 or str2 in str1:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_substring(string1,string2):\n",
    "    \n",
    "    if string1 in string2:\n",
    "#         print(string1)\n",
    "        return True,string1\n",
    "    sequence_matcher = SequenceMatcher(\n",
    "        None, string1, string2)\n",
    "    match = sequence_matcher.find_longest_match(\n",
    "        0, len(string1), 0, len(string2))\n",
    "    answer=string1[match.a:match.a+match.size]\n",
    "    if len(answer.strip())<4:\n",
    "        return False,\"\"\n",
    "#     print(answer)\n",
    "    return True ,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_with_diff(doc_name,pages,key_values,dist_,start):\n",
    "    soup,wording_reference=generate_clean_html(doc_name,pages)\n",
    "    soup_p_list=[]\n",
    "#     for p in soup.find_all('p'):\n",
    "#         text =\"\"\n",
    "#         for span in p.find_all('span'):\n",
    "#             text=text+span.text \n",
    "#         soup_p_list.append(p.text+)\n",
    "#     for divs in soup.findAll('p'):\n",
    "#         text=\"\"\n",
    "#         for span in divs.find_all('span'):\n",
    "#             text=text+''+span.text\n",
    "#         soup_p_list.append(text)\n",
    "\n",
    " \n",
    "\n",
    "#     print(\"\".join(i for i in soup_p_list))\n",
    "    soup= get_highlights(key_values,soup,start)\n",
    "    \n",
    "#     p_index=0\n",
    "#     for match in soup.findAll('p'):\n",
    "#         if p_index in highlight_index.keys():\n",
    "#             hight_value=highlight_index.get(p_index,\"\")\n",
    "\n",
    " \n",
    "\n",
    "#             for span in match.findAll('span'):\n",
    "#                 if len(span.string.strip())>1:\n",
    "\n",
    " \n",
    "\n",
    "#                     span.string=\"<ext>{}</ext>\".format(span.string)\n",
    "#         p_index+=1\n",
    "    total=0\n",
    "    for i,item in enumerate(dist_):\n",
    "        \n",
    "        p_index=-total\n",
    "#         print(p_index)\n",
    "        \n",
    "        tag,s_text=item[1].strip().split(\"$\")\n",
    "        total=total+13+len(item[0])\n",
    "#         print(total,13+len(item[0]))\n",
    "        checkFalg=False \n",
    "        total_text=\"\"\n",
    "        for p in soup.findAll('p'):\n",
    "            text=\"\"\n",
    "            \n",
    "       \n",
    "            for span in p.find_all('span'):\n",
    "                text+=span.text\n",
    "            total_text+=text\n",
    "            dist_count=text.count(\"<dist>\")\n",
    "            text=text.replace(\"<ext>\",\"\").replace(\"</ext>\",\"\")\n",
    "          \n",
    "            if p_index<=int(tag)<=p_index+len(text) and len(text.strip())>1:\n",
    "                new_div = soup.new_tag('span')\n",
    "                new_div[\"style\"]=p.span[\"style\"]\n",
    "                \n",
    "                new_div.string=\"<dist>{}</dist>\".format(item[0])\n",
    "                p.insert(dist_count, new_div)\n",
    "                \n",
    "                break\n",
    "            if start in text: \n",
    "                checkFalg=True\n",
    "            if checkFalg:\n",
    "                p_index+=len(text)\n",
    "#         print(\"***************************\")\n",
    "                \n",
    "                \n",
    "    paragraph=str(soup)\n",
    "#     paragraph = paragraph.replace(\"&lt;ext&gt;\",\"<mark>\")\n",
    "#     paragraph = paragraph.replace(\"&lt;/ext&gt;\",\"</mark>\")\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_differences(str1,str2,category):\n",
    "#     str1=str1.replace(\" MN MasterPackage 2019\",'')\n",
    "#     str2=str2.replace(\" MN MasterPackage 2019\",'')\n",
    "\n",
    " \n",
    "\n",
    "    diff_,get_=get_dif(str1,str2)\n",
    "#     print(diff_)\n",
    "    f=open(category+\"pretty.html\",\"w\",encoding=\"utf-8\")\n",
    "    f.write(diff_)\n",
    "#     print(get_)\n",
    "    removedminus=[ i for i in get_ if i[0]!=-1]\n",
    "#     print(\"\".join(i[1]for i in removedminus))\n",
    "    \n",
    "    start=\"\"\n",
    "    \n",
    "        \n",
    "    key_values={}\n",
    "    total =0\n",
    "    for index,a in enumerate(removedminus):\n",
    "        item,value=a\n",
    "\n",
    " \n",
    "\n",
    "        if item==1:\n",
    "            \n",
    "#             extra_spaces=len(value) - len(value.lstrip(' '))\n",
    "#             if extra_spaces>1:\n",
    "#                 total1=extra_spaces+total-1\n",
    "#             else:\n",
    "#                 total1=total\n",
    "            key_values[total]=value\n",
    "       \n",
    "        total+=len(value)\n",
    "    \n",
    "    \n",
    "    pre_text=\"\"\n",
    "    pre_tag=0\n",
    "    dist_=[]\n",
    "    search_highlight={}\n",
    "    flag=True\n",
    "    total=0\n",
    "    total_text=\"\"\n",
    "    \n",
    "    for item in get_:\n",
    "        \n",
    "        tag,text=item\n",
    "        total_text+=text\n",
    "        if len(text.strip())>3 and tag==0 and flag :\n",
    "            flag=False\n",
    "            start = text.split()[0]\n",
    "\n",
    " \n",
    "\n",
    "            if start.lower() in stop_words:\n",
    "                start=text[:50]\n",
    "        \n",
    "        if tag==0:\n",
    "            prev_index=tag\n",
    "            prev_value= text\n",
    "        if pre_tag==-1:\n",
    "            extra_spaces=len(value) - len(value.lstrip(' '))\n",
    "            \n",
    "            total1=extra_spaces+total           \n",
    "            dist_.append((pre_text,str(total1)+\"$\"+text))\n",
    "\n",
    " \n",
    "\n",
    "            \n",
    "        pre_tag=tag\n",
    "        pre_text=text\n",
    "\n",
    " \n",
    "\n",
    "        if tag==0 or tag==1:\n",
    "            total+=len(text)\n",
    "        \n",
    "    dist_=[i for i in dist_ if len(i[0].strip())>3 and  len(i[1].strip())>3]\n",
    "\n",
    " \n",
    "\n",
    "    return key_values,dist_,start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlights(key_values,soup,start) :\n",
    "    i= start\n",
    "#     print(i)\n",
    "    highlight_index={}\n",
    "    total=0\n",
    "    for index1,key in key_values.items():\n",
    "        content =\"\"\n",
    "        if len(key)>3:\n",
    "\n",
    "            total_len=0\n",
    "\n",
    "            flag=False\n",
    "            \n",
    "            for  p in  soup.find_all('p'):\n",
    "                a=\"\"\n",
    "                for span in p.find_all('span'):\n",
    "                    a=a+span.text \n",
    "               \n",
    "                checkFlag,answer=    long_substring(key,a)\n",
    "\n",
    "               \n",
    "                if len(a.strip())>6 and checkFlag :\n",
    "                    try:\n",
    "\n",
    "\n",
    "                        p_text_len=total_len+a.index(key)\n",
    "                        \n",
    "                        \n",
    "                        if index1==p_text_len:\n",
    "\n",
    "                            for span in p.findAll(\"span\"):\n",
    "                                span.string=\"<ext>{}</ext>\".format(span.text)\n",
    "                    \n",
    "\n",
    "                           \n",
    "\n",
    "\n",
    "                    except:\n",
    "                       \n",
    "                            \n",
    "                            \n",
    "                        try:\n",
    "\n",
    "                            if total_len>=(index1+key.index(a)-40) :\n",
    "                                \n",
    "                                for span in p.findAll('span'):\n",
    "                                    \n",
    "                                    span.string=\"<ext>{}</ext>\".format(span.text)\n",
    "                                    \n",
    "\n",
    "                                \n",
    "\n",
    "  \n",
    "                        except:\n",
    "\n",
    "                            \n",
    "                            if total_len+a.index(answer)==index1+key.index(answer):\n",
    "                                for span in p.findAll('span'):\n",
    "                                    \n",
    "                                    span.string=\"<ext>{}</ext>\".format(span.text)\n",
    "\n",
    "                                 \n",
    "                                \n",
    "\n",
    "                if a.strip() and  a.startswith(i) and not flag :\n",
    "\n",
    "                    flag=True\n",
    "                if flag :\n",
    "                    \n",
    "                    total_len+=len(a.replace(\"<ext>\",\"\").replace(\"</ext>\",\"\"))\n",
    "                    \n",
    "                    content+=a\n",
    "\n",
    "#         print(\"**************************\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# word_exclusion=word_content[0][\"exclusion\"]\n",
    "# doc_exclusion=doc_content[0][\"exclusion\"]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "\n",
    "# cosmo_endpoint=\"https://nf-poc-cdb-sql.documents.azure.com\"\n",
    "# cosmo_key=\"iEcEfrxYe0Fm9QtoxDrOpLvGsfzjowwybULlWT9Uz4XxV4RmOIAnRuLdgRFUu1LPU5Vwk3UGivRrPrxnk7083w==\"\n",
    "# client = CosmosClient(cosmo_endpoint, cosmo_key)\n",
    "# database=client.get_database_client('policy-analysis')\n",
    "# tt_documents_container=database.get_container_client('trace_files_dev')\n",
    "# trace_result_container=database.get_container_client('trace_results_dev')\n",
    "\n",
    "# # In[12]:\n",
    "\n",
    "\n",
    "# # wording_query=\"SELECT * FROM c WHERE c.file_type ='base wording' \"\n",
    "# wording_query=\"SELECT * FROM c WHERE c.title ='MasterPackage Multinational-UK-2019 (Limit of Loss S 1 & 2)-SPECIMEN-(to 31-12-2020)'\"\n",
    "\n",
    "\n",
    "# wordings = list(tt_documents_container.query_items(\n",
    "#     query=wording_query,\n",
    "#     enable_cross_partition_query=True\n",
    "# ))\n",
    "\n",
    "\n",
    "# # In[19]:\n",
    "\n",
    "\n",
    "# # document_query=\"SELECT * FROM c WHERE c.file_type ='policy document'  \"\n",
    "# document_query=\"SELECT * FROM c WHERE c.title ='test_7_extra_paragraph_document'\"\n",
    "\n",
    "\n",
    "\n",
    "# documents = list(tt_documents_container.query_items(\n",
    "#     query=document_query,\n",
    "#     enable_cross_partition_query=True\n",
    "# )) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# result_query=\"SELECT * FROM c\"\n",
    "\n",
    "\n",
    "\n",
    "# results = list(trace_result_container.query_items(\n",
    "#     query=result_query,\n",
    "#     enable_cross_partition_query=True\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wording_content=wordings[0]['contents']\n",
    "# document_content=documents[0]['contents']\n",
    "\n",
    "\n",
    "# document_name=list(document_content.keys())[0]\n",
    "# # print(\"document_name\",document_name)\n",
    "# wording_name=list(wording_content.keys())[0]\n",
    "# doc_content=document_content[document_name]\n",
    "# word_content=wording_content[wording_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_exclusion=word_content[0][\"misc\"]\n",
    "# doc_exclusion=doc_content[0][\"misc\"]\n",
    "# key=\"Section 0\"\n",
    "\n",
    "# f= open(\"final3\"+\".html\",\"w\",encoding=\"utf-8\")\n",
    "# pages=doc_exclusion[key][1]\n",
    "# pages.sort()\n",
    "# key_values,dist_,start=get_differences(word_exclusion[key][0],doc_exclusion[key][0],'exclusion')\n",
    "# # print(\"start\",start)\n",
    "# # start=\"Condition#\"\n",
    "# # print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "# # start=\"Exclusion\"\n",
    "# key_values={key:value for key,value in key_values.items() if len(value.strip())>3}\n",
    "# dist_=[dist for dist in dist_ if len(dist[1].split('$')[1].strip()) >3 ]\n",
    "# dist_\n",
    "# soup= get_html_with_diff(document_name,pages,key_values,dist_,start)\n",
    "# paragraph=str(soup)\n",
    "# paragraph = paragraph.replace(\"&lt;ext&gt;\",\"<mark>\")\n",
    "# paragraph = paragraph.replace(\"&lt;/ext&gt;\",\"</mark>\")\n",
    "# dist_\n",
    "\n",
    "\n",
    "# f.write(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "\n",
    "cosmo_endpoint=\"https://nf-poc-cdb-sql.documents.azure.com\"\n",
    "cosmo_key=\"iEcEfrxYe0Fm9QtoxDrOpLvGsfzjowwybULlWT9Uz4XxV4RmOIAnRuLdgRFUu1LPU5Vwk3UGivRrPrxnk7083w==\"\n",
    "client = CosmosClient(cosmo_endpoint, cosmo_key)\n",
    "database=client.get_database_client('policy-analysis')\n",
    "tt_documents_container=database.get_container_client('trace_files_dev')\n",
    "trace_result_container=database.get_container_client('trace_results_dev')\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "wording_query=\"SELECT * FROM c WHERE c.file_type ='base wording' \"\n",
    "\n",
    "wordings = list(tt_documents_container.query_items(\n",
    "    query=wording_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "document_query=\"SELECT * FROM c WHERE c.file_type ='policy document'  \"\n",
    "\n",
    "documents = list(tt_documents_container.query_items(\n",
    "    query=document_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "result_query=\"SELECT * FROM c\"\n",
    "\n",
    "\n",
    "\n",
    "results = list(trace_result_container.query_items(\n",
    "    query=result_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_id=[i['id'] for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for wording in wordings:\n",
    "    \n",
    "    refCode_wording=wording[\"refCode\"]\n",
    "\n",
    "    for document in documents:\n",
    "        refCode_document=document[\"refCode\"]\n",
    "        wording_start_date=wording['effectiveDate'] \n",
    "        wording_end_date=wording['expiryDate'] \n",
    "        document_start_date=document['effectiveDate']\n",
    "        if refCode_document==refCode_wording and  wording_start_date<=document_start_date<=wording_end_date:\n",
    "            k+=1\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_set=documents[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_FB1D7BDB-BD05-4275-BE2A-DB30544D967F\n",
      "REDACTED_FB1D7BDB-BD05-4275-BE2A-DB30544D967F.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_475223FC-5D58-4A82-95FE-ED41BB41874D\n",
      "REDACTED_475223FC-5D58-4A82-95FE-ED41BB41874D.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_475AD340-699B-4E02-AC8E-C1C6ADCD7D21\n",
      "REDACTED_475AD340-699B-4E02-AC8E-C1C6ADCD7D21.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_5E850792-66B4-4858-B2C7-54119326D1AE\n",
      "REDACTED_5E850792-66B4-4858-B2C7-54119326D1AE.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_ED0C3EB0-18AF-4EB3-B471-5A12F128C9FD\n",
      "REDACTED_ED0C3EB0-18AF-4EB3-B471-5A12F128C9FD.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_E23D37BA-238A-488D-B2D7-B046C7C401AC\n",
      "REDACTED_E23D37BA-238A-488D-B2D7-B046C7C401AC.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_DFCECED1-431D-4B2D-A94D-6B6ECD4B0A79\n",
      "REDACTED_DFCECED1-431D-4B2D-A94D-6B6ECD4B0A79.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_12B4EE35-3157-4A29-A36C-88E38AAD29D3\n",
      "REDACTED_12B4EE35-3157-4A29-A36C-88E38AAD29D3.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_6F644983-CE28-43D8-8D53-C69F5273A283\n",
      "REDACTED_6F644983-CE28-43D8-8D53-C69F5273A283.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_3284C99B-3F17-4BE8-855B-6F44AF08541F\n",
      "REDACTED_3284C99B-3F17-4BE8-855B-6F44AF08541F.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_EC3324E9-BA13-433E-A312-28F0EC663479\n",
      "REDACTED_EC3324E9-BA13-433E-A312-28F0EC663479.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_194291F2-D36A-4E88-9A9B-0C2BCA69A365\n",
      "REDACTED_194291F2-D36A-4E88-9A9B-0C2BCA69A365.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_31B67EC0-D391-4B14-808A-E8B1EB2EA2F5\n",
      "REDACTED_31B67EC0-D391-4B14-808A-E8B1EB2EA2F5.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_9175F33D-E184-4313-A05A-B30DD90802C5\n",
      "REDACTED_9175F33D-E184-4313-A05A-B30DD90802C5.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_AAA3876D-94CA-448B-ACDE-CB35C411EA02\n",
      "REDACTED_AAA3876D-94CA-448B-ACDE-CB35C411EA02.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_77F335B4-450D-4100-98E4-E439C6C9BE14\n",
      "REDACTED_77F335B4-450D-4100-98E4-E439C6C9BE14.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_B6C13946-C12F-48AC-818F-1460005CA51A\n",
      "REDACTED_B6C13946-C12F-48AC-818F-1460005CA51A.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_9E490D37-FE85-4DAD-91C1-A6CADAAE96CF\n",
      "REDACTED_9E490D37-FE85-4DAD-91C1-A6CADAAE96CF.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_445663C7-722D-477B-B262-ECB8E9B22D41\n",
      "REDACTED_445663C7-722D-477B-B262-ECB8E9B22D41.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_8F7A4083-0951-4E87-951E-540F92CDD0F4\n",
      "REDACTED_8F7A4083-0951-4E87-951E-540F92CDD0F4.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_AD1876B7-804A-4374-8754-ABBCC2D7A3B8\n",
      "REDACTED_AD1876B7-804A-4374-8754-ABBCC2D7A3B8.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_B93CB15E-4F00-413D-8A3F-271F67E7E778\n",
      "REDACTED_B93CB15E-4F00-413D-8A3F-271F67E7E778.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_6AE1124E-91E7-4720-8618-1060A0D243A4\n",
      "REDACTED_6AE1124E-91E7-4720-8618-1060A0D243A4.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_59150343-8D4A-4136-8EFA-7C45142EFC85\n",
      "REDACTED_59150343-8D4A-4136-8EFA-7C45142EFC85.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_806D547D-2639-4CC6-A23B-73C11333D2AA\n",
      "REDACTED_806D547D-2639-4CC6-A23B-73C11333D2AA.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_F78160BF-F218-4DD5-B018-4A8AC4C42AE5\n",
      "REDACTED_F78160BF-F218-4DD5-B018-4A8AC4C42AE5.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_FF870ADA-07AB-4BA7-BB9A-F5AAF943A07B\n",
      "REDACTED_FF870ADA-07AB-4BA7-BB9A-F5AAF943A07B.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_1A039EE8-6198-4074-A621-CA504AB473BD\n",
      "REDACTED_1A039EE8-6198-4074-A621-CA504AB473BD.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_4B2F70EB-EB26-4FAE-AE5D-CC58AACA0C9A\n",
      "REDACTED_4B2F70EB-EB26-4FAE-AE5D-CC58AACA0C9A.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_9DB195B6-899E-4397-81BD-A45C889719EB\n",
      "REDACTED_9DB195B6-899E-4397-81BD-A45C889719EB.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_7B097F7C-0071-4808-9AB2-130B8CDE3BC2\n",
      "REDACTED_7B097F7C-0071-4808-9AB2-130B8CDE3BC2.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_6A70A83C-F401-478E-9CDD-324366C95A70\n",
      "REDACTED_6A70A83C-F401-478E-9CDD-324366C95A70.pdf\n",
      "*************************************************\n",
      "MasterPackage Multinational-UK-2019 (Limit of Loss S 1 & 2)-SPECIMEN-(to 31-12-2020)REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E\n",
      "REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E.pdf\n",
      "*************************************************\n",
      "exception block MasterPackage Multinational-UK-2019 (Limit of Loss S 1 & 2)-SPECIMEN-(to 31-12-2020).pdf REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E.pdf (RequestEntityTooLarge) Message: {\"Errors\":[\"Request size is too large\"]}\n",
      "ActivityId: 395667fa-04c0-4157-8815-f2a5c8c64649, Request URI: /apps/06a56188-05a4-4903-8a6d-25444bb58c54/services/d01d635b-5366-42b8-a1ac-3cafd4a30592/partitions/abd4d695-7df1-4a3c-8536-2aa2a7c20d60/replicas/132610653028863739p/, RequestStats: \n",
      "RequestStartTime: 2021-03-24T15:30:20.1572827Z, RequestEndTime: 2021-03-24T15:30:20.1572827Z,  Number of regions attempted:1\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.11.0\n",
      "exception block1 (RequestEntityTooLarge) Message: {\"Errors\":[\"Request size is too large\"]}\n",
      "ActivityId: c2c3d0f3-9bc9-48b8-9607-a35d39b54854, Request URI: /apps/06a56188-05a4-4903-8a6d-25444bb58c54/services/d01d635b-5366-42b8-a1ac-3cafd4a30592/partitions/abd4d695-7df1-4a3c-8536-2aa2a7c20d60/replicas/132610653028863739p/, RequestStats: \n",
      "RequestStartTime: 2021-03-24T15:30:20.2172682Z, RequestEndTime: 2021-03-24T15:30:20.2172682Z,  Number of regions attempted:1\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.11.0 MasterPackage Multinational-UK-2019 (Limit of Loss S 1 & 2)-SPECIMEN-(to 31-12-2020).pdf REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E.pdf (RequestEntityTooLarge) Message: {\"Errors\":[\"Request size is too large\"]}\n",
      "ActivityId: c2c3d0f3-9bc9-48b8-9607-a35d39b54854, Request URI: /apps/06a56188-05a4-4903-8a6d-25444bb58c54/services/d01d635b-5366-42b8-a1ac-3cafd4a30592/partitions/abd4d695-7df1-4a3c-8536-2aa2a7c20d60/replicas/132610653028863739p/, RequestStats: \n",
      "RequestStartTime: 2021-03-24T15:30:20.2172682Z, RequestEndTime: 2021-03-24T15:30:20.2172682Z,  Number of regions attempted:1\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.11.0\n",
      "Chubb Multinational PDBI - UK - (from 01-05-2017 until 31-03-2018) - SpecimenREDACTED_CD4A4722-FB4C-4AF6-B50F-0B1FA30D82C7\n",
      "REDACTED_CD4A4722-FB4C-4AF6-B50F-0B1FA30D82C7.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - (from 01-05-2017 until 31-03-2018) - SpecimenREDACTED_E4EDD3B9-7BCC-467E-88F6-CC7994AD355A\n",
      "REDACTED_E4EDD3B9-7BCC-467E-88F6-CC7994AD355A.pdf\n",
      "*************************************************\n",
      "Chubb Multinational PDBI - UK - (from 01-05-2017 until 31-03-2018) - SpecimenREDACTED_359117BB-66BB-4A37-9785-842C5865DD85\n",
      "REDACTED_359117BB-66BB-4A37-9785-842C5865DD85.pdf\n",
      "*************************************************\n",
      "Chubb PDBI - UK - 04-2018 (SE - from 01-01-2019 to 31-12-20) - SPECIMENREDACTED_B3934012-921B-4D9B-8775-6E4112413588\n",
      "REDACTED_B3934012-921B-4D9B-8775-6E4112413588.pdf\n",
      "*************************************************\n",
      "Chubb PDBI - UK - 04-2018 (SE - from 01-01-2019 to 31-12-20) - SPECIMENREDACTED_85FA93C0-AAD7-434E-8036-4745A475E176\n",
      "REDACTED_85FA93C0-AAD7-434E-8036-4745A475E176.pdf\n",
      "*************************************************\n",
      "Chubb PDBI - UK - 04-2018 (SE - from 01-01-2019 to 31-12-20) - SPECIMENREDACTED_5AA94493-3BAD-4443-A56B-4C36F5B64E99\n",
      "REDACTED_5AA94493-3BAD-4443-A56B-4C36F5B64E99.pdf\n",
      "*************************************************\n",
      "MasterPackage Multinational-UK-2019 (Full Value S 1 & 2)-SPECIMEN (to 31-12-2020)REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E\n",
      "REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E.pdf\n",
      "*************************************************\n",
      "exception block MasterPackage Multinational-UK-2019 (Full Value S 1 & 2)-SPECIMEN (to 31-12-2020).pdf REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E.pdf (RequestEntityTooLarge) Message: {\"Errors\":[\"Request size is too large\"]}\n",
      "ActivityId: a9584fb1-706d-4730-8ff6-94ab600a72bb, Request URI: /apps/06a56188-05a4-4903-8a6d-25444bb58c54/services/d01d635b-5366-42b8-a1ac-3cafd4a30592/partitions/abd4d695-7df1-4a3c-8536-2aa2a7c20d60/replicas/132610653028863739p/, RequestStats: \n",
      "RequestStartTime: 2021-03-24T15:30:42.8273039Z, RequestEndTime: 2021-03-24T15:30:42.8273039Z,  Number of regions attempted:1\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.11.0\n",
      "exception block1 (RequestEntityTooLarge) Message: {\"Errors\":[\"Request size is too large\"]}\n",
      "ActivityId: 4e33c875-0fa4-4c10-a54f-476966061572, Request URI: /apps/06a56188-05a4-4903-8a6d-25444bb58c54/services/d01d635b-5366-42b8-a1ac-3cafd4a30592/partitions/abd4d695-7df1-4a3c-8536-2aa2a7c20d60/replicas/132610653028863739p/, RequestStats: \n",
      "RequestStartTime: 2021-03-24T15:30:42.8872699Z, RequestEndTime: 2021-03-24T15:30:42.8872699Z,  Number of regions attempted:1\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.11.0 MasterPackage Multinational-UK-2019 (Full Value S 1 & 2)-SPECIMEN (to 31-12-2020).pdf REDACTED_9EA07FB4-511F-4DD3-8FFB-636C21AC5F0E.pdf (RequestEntityTooLarge) Message: {\"Errors\":[\"Request size is too large\"]}\n",
      "ActivityId: 4e33c875-0fa4-4c10-a54f-476966061572, Request URI: /apps/06a56188-05a4-4903-8a6d-25444bb58c54/services/d01d635b-5366-42b8-a1ac-3cafd4a30592/partitions/abd4d695-7df1-4a3c-8536-2aa2a7c20d60/replicas/132610653028863739p/, RequestStats: \n",
      "RequestStartTime: 2021-03-24T15:30:42.8872699Z, RequestEndTime: 2021-03-24T15:30:42.8872699Z,  Number of regions attempted:1\n",
      ", SDK: Microsoft.Azure.Documents.Common/2.11.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_query=\"SELECT * FROM c\"\n",
    "\n",
    "results = list(trace_result_container.query_items(\n",
    "    query=result_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "\n",
    "results_id=[i['id'] for i in results]\n",
    "\n",
    "final_list=[]\n",
    "for wording in wordings:\n",
    "    \n",
    "    refCode_wording=wording[\"refCode\"]\n",
    "\n",
    "    for document in document_set:\n",
    "        refCode_document=document[\"refCode\"]\n",
    "        wording_start_date=wording['effectiveDate'] \n",
    "        wording_end_date=wording['expiryDate'] \n",
    "        document_start_date=document['effectiveDate']\n",
    "        try:\n",
    "            if refCode_document==refCode_wording and  wording_start_date<=document_start_date<=wording_end_date:            \n",
    "                wording_content=wording['contents']\n",
    "                document_content=document['contents']\n",
    "                \n",
    "\n",
    "\n",
    "                document_name=list(document_content.keys())[0]\n",
    "\n",
    "                wording_name=list(wording_content.keys())[0]\n",
    "                id_name=wording_name.replace(\".pdf\",\"\")+document_name.replace(\".pdf\",\"\")\n",
    "                print(id_name)\n",
    "                \n",
    "                \n",
    "                if id_name in results_id:\n",
    "                    print('Already Exist........')\n",
    "                    continue\n",
    "\n",
    "                final_list.append([wording_name,document_name,refCode_document,refCode_wording,wording[\"effectiveDate\"],document[\"effectiveDate\"]])\n",
    "\n",
    "                doc_content=document_content[document_name]\n",
    "                word_content=wording_content[wording_name]\n",
    "\n",
    "                print(document_name)\n",
    "                matches={}\n",
    "\n",
    "                total_category_MR=[]\n",
    "                word_content[0].pop(\"pages\", None)\n",
    "                doc_content[0].pop(\"pages\", None)\n",
    "\n",
    "                for category in word_content[0].keys():\n",
    "\n",
    "                    category_content_wording=word_content[0][category]\n",
    "                    category_content_document=doc_content[0][category]\n",
    "                    results=[]\n",
    "                    total_section_MR=[]\n",
    "                    total_section_score=0\n",
    "                    total_sections=0\n",
    "    #                 print(category_content_document.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    for section_w in category_content_wording.keys():\n",
    "                        new_dict ={}\n",
    "                        keyFlag=False\n",
    "                        ##keys are present in both the wording and document\n",
    "\n",
    "                        if section_w not in category_content_document.keys():\n",
    "                            for section_d in category_content_document.keys():\n",
    "\n",
    "                                if section_w in section_d or section_d in section_w:\n",
    "                                    key_wording=section_w\n",
    "                                    key_document=section_d\n",
    "                                    keyFlag=True\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            key_wording=section_w\n",
    "                            key_document=section_w\n",
    "\n",
    "                            keyFlag=True\n",
    "\n",
    "                        if keyFlag:\n",
    "    #                         print(key_wording,\"*\",key_document,category)\n",
    "                            section_content,pages=category_content_document[key_document]\n",
    "\n",
    "                            wording_section_value,wording_section_pages=category_content_wording[key_wording]\n",
    "                            pages.sort()\n",
    "\n",
    "                            wording_section_pages.sort()\n",
    "                            ##when wording pages and document pages are there we will compare\n",
    "                            if len(pages)>=1 and len(wording_section_pages)>=1:\n",
    "\n",
    "                                key_values,dist_,start=get_differences(wording_section_value,section_content,category)\n",
    "                                dist_=[dist for dist in dist_ if len(dist[1].split('$')[1].strip()) >3 ]\n",
    "                                key_values={key:value for key,value in key_values.items() if len(value.strip())>3 and refCode_document!=value}\n",
    "\n",
    "                                score=SequenceMatcher(None,wording_section_value,section_content).ratio()*100\n",
    "                                if not key_values and not dist_:\n",
    "                                    score=100\n",
    "                                if \"SPECIAL EXTENSIONS APPLICABLE\" in  section_w :\n",
    "                                    score=100\n",
    "                                    \n",
    "\n",
    "                                matchType,relevance=search_relevance(score)\n",
    "                                total_section_score+=score\n",
    "                                total_sections+=1\n",
    "                                new_dict[\"section\"]=key_document\n",
    "\n",
    "\n",
    "                                if (key_values or  dist_) and score <99:\n",
    "\n",
    "\n",
    "                                    content_html= get_html_with_diff(document_name,pages,key_values,dist_,start)\n",
    "\n",
    "\n",
    "                                    new_dict[\"content\"]=content_html\n",
    "\n",
    "                                new_dict[\"relevance\"] = relevance\n",
    "                                new_dict[\"matchType\"] = matchType\n",
    "                                new_dict[\"score\"]=score\n",
    "                                new_dict[\"document_page\"]=pages[0]\n",
    "\n",
    "\n",
    "                                new_dict[\"wording_page\"]=wording_section_pages[0]\n",
    "\n",
    "\n",
    "                                results.append(new_dict)\n",
    "                                total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\"matchType\":new_dict[\"matchType\"]})\n",
    "\n",
    "                            else:\n",
    "                                ##if there are documnet pages\n",
    "                                ##elif there are wording pages\n",
    "                                if len(pages)>=1:\n",
    "                                    new_dict[\"section\"]=key_document\n",
    "                                    new_dict[\"content\"]=\"<ext>{}</ext>\".format(get_html(document_name,pages))\n",
    "\n",
    "                                    new_dict[\"document_page\"]=pages[0]\n",
    "                                    new_dict[\"relevance\"] = \"\"\n",
    "                                    new_dict[\"matchType\"] = \"no\"\n",
    "                                    new_dict[\"score\"]=0\n",
    "\n",
    "                                    results.append(new_dict)\n",
    "                                    total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\"matchType\":new_dict[\"matchType\"]})\n",
    "\n",
    "\n",
    "                                elif len(wording_section_pages)>=1:\n",
    "                                    new_dict[\"section\"]=key_wording\n",
    "                                    new_dict[\"content\"]=\"<dist>{}</dist>\".format(get_html(wording_name,wording_section_pages,doc_path=r\"../../../Trace_files/PDBI/wordings\"))\n",
    "\n",
    "                                    new_dict[\"wording_page\"]=wording_section_pages[0]\n",
    "                                    new_dict[\"relevance\"] = \"\"\n",
    "                                    new_dict[\"matchType\"] = \"no\"\n",
    "                                    new_dict[\"score\"]=0\n",
    "\n",
    "                                    results.append(new_dict)\n",
    "                                    total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\"matchType\":new_dict[\"matchType\"]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        ###if the key it not there in document we should show wording pages with \"No match\"\n",
    "                        if not keyFlag:\n",
    "                            \n",
    "                            _,pages_word=category_content_wording[section_w]\n",
    "\n",
    "                            if len(pages_word)>=1:\n",
    "                                pages_word.sort()\n",
    "                                new_dict[\"section\"]=section_w\n",
    "\n",
    "\n",
    "                                new_dict[\"content\"]=\"<dist>{}</dist>\".format(get_html(wording_name,pages_word,doc_path=r\"../../../Trace_files/PDBI/wordings\"))\n",
    "                                new_dict[\"relevance\"] = \"\"\n",
    "                                new_dict[\"matchType\"] = \"no\"\n",
    "                                new_dict[\"score\"]=0\n",
    "                                new_dict[\"wording_page\"]=pages_word[0]\n",
    "                                results.append(new_dict)\n",
    "                                total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\n",
    "                                                                 \"matchType\":new_dict[\"matchType\"]})\n",
    "                    total_section_matchType,total_section_relevance=getMatchTypeRelevance(total_section_MR)\n",
    "                    matches[category]= {\"results\":results,\n",
    "                                        \"matchType\": total_section_matchType,\"relevance\":total_section_relevance}\n",
    "                    total_category_MR.append({\"relevance\":total_section_relevance,\n",
    "                                                         \"matchType\":total_section_matchType})  \n",
    "\n",
    "                total_category_matchType,total_category_relevance=getMatchTypeRelevance(total_category_MR)\n",
    "                \n",
    "                insert_item={\n",
    "                              \"policyDetails\": {\n",
    "                                    \"policyNo\": document[\"policyNo\"],\n",
    "                                    \"expiryDate\": document[ \"expiryDate\"],\n",
    "                                    \"effectiveDate\": document[\"effectiveDate\"]\n",
    "                                  },\n",
    "                            \"title\": document_name.replace(\".pdf\",\"\"),\n",
    "                            \"id\":id_name,\n",
    "                            \"wordingTitle\":wording_name.replace(\".pdf\",\"\"),\n",
    "                            \"matchType\": total_category_matchType,\n",
    "\n",
    "                            \"relevance\":total_category_relevance,\n",
    "\n",
    "                            \"refCode\": document[\"refCode\"],\n",
    "\n",
    "                             \"country\":\"GB\",\n",
    "                            \"lob\":document[\"lob\"],\n",
    "                            \"matches\": matches\n",
    "                            }\n",
    "                f=open(\"bb\"+document['title']+\".json\",\"w\")\n",
    "\n",
    "                f.write(json.dumps(insert_item))\n",
    "                print(\"*************************************************\")  \n",
    "                try:\n",
    "                    trace_result_container.create_item(body=insert_item)\n",
    "                except Exception as e:\n",
    "                    print(\"exception block\",wording_name,document_name,e)\n",
    "                    try:\n",
    "                        trace_result_container.replace_item(item=document_name, body=insert_item)\n",
    "                    except Exception as e1:\n",
    "                        print(\"exception block1\",e1,wording_name,document_name,e1)\n",
    "        except Exception as err:\n",
    "            print(\"higher level exception\",err,wording_name,document_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_keys={}\n",
    "# for key_w in word_exclusion.keys():\n",
    "#     keyFlag=False\n",
    "#     key_wording=key_w\n",
    "#     if key_w not in doc_exclusion.keys():\n",
    "#         for key_doc in doc_exclusion.keys():\n",
    "\n",
    "#             if key_w in key_doc or key_doc in key_w:\n",
    "#                 key_wording=key_w\n",
    "#                 key_document=key_doc\n",
    "#                 keyFlag=True\n",
    "#                 print(key_wording,\"*\",key_document)\n",
    "#     else:\n",
    "#         key_wording=key_w\n",
    "#         key_document=key_w\n",
    "#         print(key_wording,\"*\",key_document)\n",
    "#         keyFlag=True\n",
    "        \n",
    "#     if not keyFlag:\n",
    "#         print(key_w)\n",
    "        \n",
    "    \n",
    "    \n",
    "                    \n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_exclusion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-e0eb783a0ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_exclusion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_exclusion' is not defined"
     ]
    }
   ],
   "source": [
    "word_exclusion[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_exclusion[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REDACTED_0BCBCCDF-0CC2-465E-A32A-41818E5894EC.pdf'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={\"Conditions\":\"abc\"}\n",
    "b={\"special Conditions\":\"def\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "partial_ratio = fuzz.partial_ratio( \"special Conditions\",\" Section 4 Conditions\")\n",
    "print(partial_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                         if total_sections!=0:\n",
    "#                             total_section_score= round(total_section_score/total_sections,2)\n",
    "\n",
    "#                         total_section_matchType=\"Full\"\n",
    "#                         if total_section_score<99.5:\n",
    "#                             total_section_matchType=\"partial\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
