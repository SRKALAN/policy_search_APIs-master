{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "def search_relevance (score):\n",
    "        match,relevance = \"no\",\"\"\n",
    "        if score>=99:\n",
    "            match,relevance = \"full\",\"\"\n",
    "        elif score >=90:\n",
    "            match,relevance = \"partial\",\"high\"\n",
    "        elif score >=70:\n",
    "            match,relevance = \"partial\",\"medium\"\n",
    "        elif 0<score <70:\n",
    "            match,relevance = \"partial\",\"low\"\n",
    "        return match,relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import fitz\n",
    "\n",
    "from pdf2txt import convert_pdf\n",
    "from bs4 import BeautifulSoup,NavigableString\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchTypeRelevance(a) : \n",
    "\n",
    "    matchtype_agg=\"\"\n",
    "    relevance_agg=\"\"\n",
    "    for i in a:\n",
    "        matchtype = i.get(\"matchType\",\"\")\n",
    "        relevance = i.get(\"relevance\",\"\")\n",
    "        \n",
    "        \n",
    "        if matchtype==\"partial\" :\n",
    "            \n",
    "            if matchtype_agg not in(\"no\"):\n",
    "                matchtype_agg=matchtype\n",
    "            elif matchtype_agg==\"\":\n",
    "                matchtype_agg=matchtype\n",
    "                \n",
    "            \n",
    "                \n",
    "        elif matchtype==\"full\" :\n",
    "            if matchtype_agg not in(\"partial\",\"no\"):\n",
    "                matchtype_agg=matchtype\n",
    "            elif matchtype_agg==\"\":\n",
    "                matchtype_agg=matchtype\n",
    "        elif matchtype==\"no\":\n",
    "            matchtype_agg=matchtype\n",
    "        \n",
    "        if relevance==\"low\" :\n",
    "            relevance_agg=relevance\n",
    "        elif relevance==\"medium\" :\n",
    "            \n",
    "            if relevance_agg not in(\"low\"):\n",
    "                relevance_agg=relevance\n",
    "            elif relevance_agg==\"\":\n",
    "                relevance_agg=relevance\n",
    "            \n",
    "        elif relevance==\"high\" :\n",
    "            \n",
    "            if relevance_agg not in(\"medium\",\"low\"):\n",
    "                relevance_agg=relevance\n",
    "            elif relevance_agg==\"\":\n",
    "                relevance_agg=relevance\n",
    "            \n",
    "    if matchtype_agg==\"no\" or matchtype_agg==\"full\":\n",
    "        relevance_agg=\"\"\n",
    "   \n",
    "    return matchtype_agg,relevance_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabula import read_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(doc_name,pages,doc_path=r\"C:\\Users\\LATEJAS\\Videos\\PDBI\"):\n",
    "    paragraph=\"\"\n",
    "    doc=fitz.open(os.path.join(doc_path,doc_name))\n",
    "    for page_no in pages:\n",
    "\n",
    "        page = doc.loadPage(page_no-1)\n",
    "        paragraph=paragraph+page.getText(\"html\")+\"\\n\"\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_html(doc_name,pages,doc_path=r\"C:\\Users\\LATEJAS\\Videos\\PDBI\"):\n",
    "    fitz_doc = fitz.open(os.path.join(doc_path,doc_name))\n",
    "    html_full=''\n",
    "    wording_ref=[]\n",
    "    footer_dict={}\n",
    "    total_pages=len(fitz_doc)\n",
    "#     print(total_pages)\n",
    "    for page_no in pages:\n",
    "        page = fitz_doc.loadPage(page_no-1)\n",
    "        \n",
    "\n",
    "        page_html=page.getText(\"html\")\n",
    "       \n",
    "        page_html=page_html.replace('position','page:{};position'.format(page_no))\n",
    "        soup = BeautifulSoup(page_html, 'html5lib')\n",
    "        toc_page_no=[]\n",
    "        dot_count=0\n",
    "        introflag=False\n",
    "\n",
    "        for p in soup.findAll('p'):        \n",
    "            p_str=str(p)\n",
    "            dot_count+=p_str.count('.')\n",
    "\n",
    "            try:        \n",
    "                xcord=int(x_cordinate_Re.findall(p_str)[0])\n",
    "            except:\n",
    "                xcord=0 \n",
    "            try:\n",
    "                ycord=int(y_cordinate_Re.findall(p_str)[0])\n",
    "            except:\n",
    "                ycord=0\n",
    "\n",
    "            for span in p.findAll('span'):\n",
    "                text=span.text.strip()\n",
    "                try:\n",
    "                    toc_page=int(span.text)\n",
    "\n",
    "\n",
    "                    if page_no< total_pages and xcord>500:\n",
    "                        toc_page_no.append(toc_page)                   \n",
    "                except Exception as err:\n",
    "                    pass\n",
    "                try:\n",
    "                    font_size=float(font_size_RE.search(p_str).group(1))\n",
    "                except Exception as err:\n",
    "                    print(err)\n",
    "                    font_size=8\n",
    "\n",
    "                if xcord < 150 and ycord >720 and font_size <10 :\n",
    "\n",
    "\n",
    "                    wording_ref.append(text)\n",
    "                if  ycord >720  and font_size <10 :\n",
    "                    footer_dict[text]=ycord\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "        if len(toc_page_no) >10 or dot_count>1500 :\n",
    "#             print('table of content',page_no)\n",
    "            continue\n",
    "        if 'img' in page_html:\n",
    "#             print('Image',page_no)\n",
    "            continue\n",
    "        if 'Introduction' in page_html and 'www.chubb.com' in page_html:\n",
    "#             print('Intro',page_no)\n",
    "            continue\n",
    "\n",
    "#         if page_no <=16:\n",
    "#             df=read_pdf(file,  pages=page_no)\n",
    "#             if df:\n",
    "#                 print('TABLE FOUND IN ',page_no)\n",
    "#                 continue\n",
    "\n",
    "\n",
    "        html_full+=page_html\n",
    "\n",
    "    wording_ref=[i for i in wording_ref if len(i)>2]\n",
    "    wording_reference_p_text=max(set(wording_ref), key = wording_ref.count)\n",
    "    footer_y_cord=footer_dict[wording_reference_p_text]-20\n",
    "    wording_reference=[i for i in wording_reference_p_text.split('    ') if len(i)>0][0]\n",
    "    \n",
    "    html_full=html_full.replace('width:-2pt;height:-2pt','width:596pt;height:842pt')\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    soup = BeautifulSoup(html_full, 'html5lib')\n",
    "\n",
    "    for p in soup.findAll('p'):\n",
    "        p_str=str(p)\n",
    "        text=p.text.strip()\n",
    "\n",
    "\n",
    "        try:        \n",
    "            xcord=int(x_cordinate_Re.findall(p_str)[0])\n",
    "        except:\n",
    "            xcord=0 \n",
    "        try:\n",
    "            ycord=int(y_cordinate_Re.findall(p_str)[0])\n",
    "        except:\n",
    "            ycord=0\n",
    "\n",
    "        try:\n",
    "            font_size=float(font_size_RE.search(p_str).group(1))\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            font_size=8\n",
    "        if \"policy number:\" in p.text.lower() and ycord <100:\n",
    "#             print(p.text)\n",
    "            p.string=''\n",
    "        if  font_size >40 :\n",
    "#             print(p.text,'WATERMARKKK')\n",
    "            p.string=''\n",
    "    \n",
    "        if len(p.text)>4:    \n",
    "            if ycord >footer_y_cord and (font_size <8 or (p.text.strip()==wording_reference.strip())):\n",
    "#                 print(p.text,'refff')\n",
    "                p.string=''\n",
    "        if font_size < 7 :\n",
    "#             print(p.text,'less than 7 ')\n",
    "            p.string=''\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "    return soup,wording_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_content = str(soup1)\n",
    "# Html_file= open(\"doc_mod.html\",\"w\",encoding='utf-8')\n",
    "# Html_file.write(html_content)\n",
    "# Html_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pointsRE_heading=re.compile('(?:\\s*\\([a-z]{1,3}\\)|[A-Z]{1}\\s+[a-zA-Z0-9_\\s]{5})')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "y_cordinate_Re=re.compile(';top:(\\d{1,5})pt')  #top:766pt\n",
    "x_cordinate_Re=re.compile(';left:(\\d{1,5})pt')\n",
    "height_re=re.compile('height:(\\d{1,5})pt')\n",
    "font_size_RE = re.compile(\"font-size:(.*?)pt\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_match_patch as dmp_module\n",
    "\n",
    "dmp = dmp_module.diff_match_patch()\n",
    "\n",
    "\n",
    "def get_dif(str1,str2):\n",
    "    dmp.Diff_Timeout = 0\n",
    "    \n",
    "#     if 10000 <len(str2)<20000:\n",
    "#          dmp.Diff_Timeout = 20\n",
    "#     elif 20000<=len(str2)<25000:\n",
    "#         dmp.Diff_Timeout = 60\n",
    "#     elif 25000<=len(str2)<30000:\n",
    "#         dmp.Diff_Timeout = 120\n",
    "#     elif 30000<=len(str2)<35000:\n",
    "#         dmp.Diff_Timeout = 180\n",
    "#     elif len(str2)>35000:\n",
    "#         dmp.Diff_Timeout = 240\n",
    "    \n",
    "\n",
    "#     print(        dmp.Diff_Timeout )   \n",
    "    d = dmp.diff_main(str1, str2)\n",
    "    dmp.diff_cleanupSemantic(d)\n",
    "\n",
    "    paragraph=dmp.diff_prettyHtml( d)\n",
    "    return paragraph,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_match(str1,str2):\n",
    "\n",
    "    if str1 in str2 or str2 in str1:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_substring(string1,string2):\n",
    "    \n",
    "    if string1 in string2:\n",
    "#         print(string1)\n",
    "        return True,string1\n",
    "    sequence_matcher = SequenceMatcher(\n",
    "        None, string1, string2)\n",
    "    match = sequence_matcher.find_longest_match(\n",
    "        0, len(string1), 0, len(string2))\n",
    "    answer=string1[match.a:match.a+match.size]\n",
    "#     if len(answer.strip())<4:\n",
    "#         return False,\"\"\n",
    "#     print(answer)\n",
    "    return True ,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# word_exclusion=word_content[0][\"exclusion\"]\n",
    "# doc_exclusion=doc_content[0][\"exclusion\"]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "\n",
    "cosmo_endpoint=\"https://nf-poc-cdb-sql.documents.azure.com\"\n",
    "cosmo_key=\"iEcEfrxYe0Fm9QtoxDrOpLvGsfzjowwybULlWT9Uz4XxV4RmOIAnRuLdgRFUu1LPU5Vwk3UGivRrPrxnk7083w==\"\n",
    "client = CosmosClient(cosmo_endpoint, cosmo_key)\n",
    "database=client.get_database_client('policy-analysis')\n",
    "tt_documents_container=database.get_container_client('trace_files_dev')\n",
    "trace_result_container=database.get_container_client('trace_results_dev')\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# wording_query=\"SELECT * FROM c WHERE c.file_type ='base wording' \"\n",
    "wording_query=\"SELECT * FROM c WHERE c.title ='Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMEN'\"\n",
    "\n",
    "wordings = list(tt_documents_container.query_items(\n",
    "    query=wording_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# document_query=\"SELECT * FROM c WHERE c.file_type ='policy document'  \"\n",
    "document_query=\"SELECT * FROM c WHERE c.title ='REDACTED_3284C99B-3F17-4BE8-855B-6F44AF08541F'\"\n",
    "\n",
    "documents = list(tt_documents_container.query_items(\n",
    "    query=document_query,\n",
    "    enable_cross_partition_query=True\n",
    ")) \n",
    "\n",
    "\n",
    "\n",
    "result_query=\"SELECT * FROM c\"\n",
    "\n",
    "\n",
    "\n",
    "results = list(trace_result_container.query_items(\n",
    "    query=result_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "wording_content=wordings[0]['contents']\n",
    "document_content=documents[0]['contents']\n",
    "\n",
    "\n",
    "document_name=list(document_content.keys())[0]\n",
    "# print(\"document_name\",document_name)\n",
    "wording_name=list(wording_content.keys())[0]\n",
    "doc_content=document_content[document_name]\n",
    "word_content=wording_content[wording_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_with_diff(doc_name,pages,key_values,dist_,start,edgevalues):\n",
    "    soup,wording_reference=generate_clean_html(doc_name,pages)\n",
    "    soup_p_list=[]\n",
    "\n",
    "    soup= get_highlights(key_values,soup,start)\n",
    "    \n",
    "\n",
    "    total=0\n",
    "    for i,item in enumerate(dist_):\n",
    "        \n",
    "        \n",
    "        p_index=-total\n",
    "\n",
    "        \n",
    "        tag,s_text=item[1].strip().split(\"$\")\n",
    "        total=total+13+len(item[0])\n",
    "        searchtext=\" \".join(s_text.split()[:4])\n",
    "\n",
    "        checkFalg=False \n",
    "        total_text=\"\"\n",
    "        for p in soup.findAll('p'):\n",
    "            text=p.text\n",
    "\n",
    "            total_text+=text\n",
    "            \n",
    "            dist_count=text.count(\"<dist>\")\n",
    "            original_text=text\n",
    "            text=text.replace(\"<ext>\",\"\").replace(\"</ext>\",\"\")\n",
    "            \n",
    "\n",
    "            if p_index<=int(tag)<=p_index+len(text)  and len(s_text.strip())<3:\n",
    "                p.string =\"<dist>{}</dist>\".format(item[0])\n",
    "                break\n",
    "            if p_index<=int(tag)<=p_index+len(text) and len(text.strip())>1 :\n",
    "                \n",
    "                \n",
    "                flag,answer=long_substring(searchtext,text)\n",
    "               \n",
    "                if flag:\n",
    "                    p.string=add_dist_tag(original_text,answer,item[0])\n",
    "                    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "                    break\n",
    "            if start in text: \n",
    "                checkFalg=True\n",
    "            if checkFalg:\n",
    "                p_index+=len(text)\n",
    "            \n",
    "     \n",
    "\n",
    "                \n",
    "                \n",
    "    paragraph=str(soup)\n",
    "    if len(edgevalues)>=2:\n",
    "        print(edgevalues[0][1])\n",
    "        find_string=edgevalues[0][1].split()[-1]\n",
    "        value = paragraph.rfind(find_string)\n",
    "        if value!=-1:\n",
    "            value+=len(find_string)\n",
    "            paragraph= paragraph[:value] + \"&lt;dist&gt;{}&lt;/dist&gt;\".format(edgevalues[1][1]) + paragraph[value + 1:]\n",
    "            \n",
    "    paragraph=paragraph.replace(\"color:#ffffff\",\"\")\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlights(key_values,soup,start) :\n",
    "    i= start\n",
    "#     print(i)\n",
    "    highlight_index={}\n",
    "    total=0\n",
    "    for index1,key in key_values.items():\n",
    "        content =\"\"\n",
    "        total_len=0\n",
    "\n",
    "        flag=False\n",
    "\n",
    "        key_count=0\n",
    "        for  p in  soup.find_all('p'):\n",
    "            a=p.text\n",
    "\n",
    "            replace_text=p.text.replace(\"<ext>\",\"\").replace(\"</ext>\",\"\")\n",
    "\n",
    "\n",
    "\n",
    "            checkFlag,answer=    long_substring(key.strip(),replace_text)\n",
    "            \n",
    "            if len(a.strip())>6 and checkFlag :\n",
    "\n",
    "\n",
    "                try:\n",
    "\n",
    "\n",
    "                    p_text_len=total_len+a.index(key,key_count)\n",
    "\n",
    "\n",
    "                    if index1==p_text_len:\n",
    "                        new_tag = soup.new_tag(\"span\")\n",
    "                        p.string=a.replace(key,\"<ext>{}</ext>\".format(key))\n",
    "                        key_count+=len(key)\n",
    "\n",
    "\n",
    "                except:\n",
    "\n",
    "\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        if index1+key.index(a,key_count)-40<=total_len<=index1+key.index(a,key_count) :\n",
    "                            p.string=a.replace(a,\"<ext>{}</ext>\".format(a))\n",
    "                            key_count+=len(a)\n",
    "                            print(p.string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        if total_len+replace_text.index(answer)==index1+key.index(answer) and answer is not None:\n",
    "\n",
    "                            p.string=a.replace(answer,\"<ext>{}</ext>\".format(answer))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if a.strip() and  a.startswith(start) and not flag :\n",
    "\n",
    "                flag=True\n",
    "            if flag :\n",
    "\n",
    "                total_len+=len(a.replace(\"<ext>\",\"\").replace(\"</ext>\",\"\"))\n",
    "\n",
    "                content+=a\n",
    "\n",
    "#         print(\"p\",content)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_dist_tag(b,a,add):\n",
    "\n",
    "\n",
    "    tag_re=re.compile(r'</*ext>|</*dist>')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    match_d={}\n",
    "    matches = tag_re.finditer(b)\n",
    "    for match in matches:\n",
    "        match_d[match.start()]=(match.group(),len(match.group()))\n",
    "\n",
    "\n",
    "\n",
    "    clean_b=tag_re.sub('',b)\n",
    "    replace_string=\"<dist>{}</dist>\".format(add)\n",
    "    new_added=clean_b.replace(a,replace_string+a)\n",
    "    replace_pos=new_added.find(replace_string)\n",
    "    prev_tag_len=0\n",
    "    for key,value in match_d.items():\n",
    "        if key >=replace_pos+prev_tag_len:\n",
    "            key=key+len(replace_string)   \n",
    "\n",
    "\n",
    "\n",
    "        new_added=new_added[:key] + value[0] + new_added[key:]\n",
    "        prev_tag_len+=len(value[0])\n",
    "    return new_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_differences(str1,str2,category):\n",
    "\n",
    "\n",
    "    diff_,get_=get_dif(str1,str2)\n",
    "   \n",
    "    f=open(category+\"pretty.html\",\"w\",encoding=\"utf-8\")\n",
    "    f.write(diff_)\n",
    "\n",
    "    removedminus=[ i for i in get_ if i[0]!=-1]\n",
    "\n",
    "    \n",
    "    start=\"\"\n",
    "    \n",
    "        \n",
    "    key_values={}\n",
    "    total =0\n",
    "    for index,a in enumerate(removedminus):\n",
    "        item,value=a\n",
    "\n",
    "        if item==1:\n",
    "            \n",
    "\n",
    "            key_values[total]=value\n",
    "       \n",
    "        total+=len(value)\n",
    "    \n",
    "    \n",
    "    pre_text=\"\"\n",
    "    pre_tag=0\n",
    "    dist_=[]\n",
    "    search_highlight={}\n",
    "    flag=True\n",
    "    total=0\n",
    "    total_text=\"\"\n",
    "    \n",
    "    for item in get_:\n",
    "        \n",
    "        tag,text=item\n",
    "        total_text+=text\n",
    "        if len(text.strip())>3 and (tag==0 or tag==1) and flag :\n",
    "            flag=False\n",
    "            start = text.split()[0]\n",
    "#             print(start)\n",
    "            if start.lower() in stop_words:\n",
    "                start=text[:50]\n",
    "        \n",
    "        if tag==0:\n",
    "            prev_index=tag\n",
    "            prev_value= text\n",
    "        if pre_tag==-1:\n",
    "            extra_spaces=len(value) - len(value.lstrip(' '))\n",
    "            \n",
    "            total1=extra_spaces+total           \n",
    "            dist_.append((pre_text,str(total1)+\"$\"+text))\n",
    "\n",
    "            \n",
    "        pre_tag=tag\n",
    "        pre_text=text\n",
    "\n",
    "        if tag==0 or tag==1:\n",
    "            total+=len(text)\n",
    "        \n",
    "    dist_=[dist for dist in dist_ if len(dist[1].split('$')[1].strip()) >=1 and len(dist[0].strip()) >3 ]\n",
    "    key_values={key:value for key,value in key_values.items() if len(value.strip())>=3 and refCode_document!=value}\n",
    "    get_= [(i[0],i[1]) for i in get_ if len(i[1].strip())>1]\n",
    "    edgevalues=\"\"\n",
    "    if get_[-1][0]==-1:\n",
    "        edgevalues=get_[-2:]\n",
    "    print(\"edgevalues\",edgevalues)\n",
    "    return key_values,dist_,start,edgevalues\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edgevalues \n",
      "<ext>Business Interruption: </ext>\n",
      "<ext>Property Damage: </ext>\n",
      "<ext>The Company shall not be liable under this Extension in respect of damage to equipment: </ext>\n",
      "<ext>in respect of which there is no guarantee, maintenance, rental, hire or lease agreement in force </ext>\n",
      "<ext>providing a minimum service of on-call remedial or corrective maintenance at inclusive cost, other </ext>\n",
      "<ext>than damage occasioned by or happening through: </ext>\n",
      "<ext>the negligence of the Insured, or </ext>\n",
      "<ext>the failure or fluctuation of the supply of electricity to the equipment; </ext>\n",
      "<ext>caused by or occasioned through: </ext>\n",
      "<ext>the intentional act or wilful neglect of the Insured; </ext>\n",
      "<ext>wear and tear, gradual deterioration, gradually developing defects, corrosion, rust or </ext>\n",
      "<ext>any testing, repairing, adjusting, servicing or maintenance operation. </ext>\n",
      "<ext>Business Interruption: </ext>\n",
      "<ext>The Company shall not be liable under this Extension in respect of:- </ext>\n",
      "<ext>loss sustained during the first 24 hours following damage to equipment in respect of which there is no </ext>\n",
      "<ext>guarantee, maintenance, rental, hire or lease agreement in force providing a minimum service of on-</ext>\n",
      "<ext>call remedial or corrective maintenance at inclusive cost, other than damage occasioned by or </ext>\n",
      "<ext>happening through the failure or fluctuation of the supply of electricity to the equipment </ext>\n",
      "<ext>loss resulting from damage caused by or occasioned through:- </ext>\n",
      "<ext>the intentional act or wilful neglect of the Insured; </ext>\n",
      "<ext>wear and tear gradual deterioration, gradually developing defects, corrosion, rust, or </ext>\n",
      "<ext>scratching or chipping of painted or polished surfaces; </ext>\n",
      "<ext>loss or destruction of or damage to money contained in coin operated gaming or amusement devices or vending </ext>\n",
      "<ext>machines; </ext>\n",
      "<ext>loss or destruction of or damage to Money contained in any vehicle while left unattended </ext>\n",
      "<ext>any loss, destruction or damage:- </ext>\n",
      "<ext>arising outside the Territorial Limits; </ext>\n",
      "<ext>notwithstanding Exclusion 1.f. arising from dishonesty on the part of any employee of the Insured not </ext>\n",
      "<ext>occasioned by errors or omissions; </ext>\n",
      "<ext>resulting from a safe or strong room being opened by a key if the key be left in the Premises out of </ext>\n",
      "<ext>Business Hours; </ext>\n",
      "<ext>recoverable from any security carrier. </ext>\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92731"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_exclusion=word_content[0][\"exclusion\"]\n",
    "doc_exclusion=doc_content[0][\"exclusion\"]\n",
    "key=\"Section 1 Property Damage and Section 2 Business Interruption\"\n",
    "\n",
    "f= open(\"final2\"+\".html\",\"w\",encoding=\"utf-8\")\n",
    "pages=doc_exclusion[key][1]\n",
    "pages.sort()\n",
    "\n",
    "key_values,dist_,start,edgevalues=get_differences(word_exclusion[key][0],doc_exclusion[key][0],'exclusion')\n",
    "# start=\"Data\"\n",
    "# print(\"start\",start)\n",
    "# start=\"Condition#\"\n",
    "# print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "# start=\"Exclusion\"\n",
    "\n",
    "\n",
    "soup= get_html_with_diff(document_name,pages,key_values,dist_,start,edgevalues)\n",
    "paragraph=str(soup)\n",
    "paragraph = paragraph.replace(\"&lt;ext&gt;\",\"<mark>\")\n",
    "paragraph = paragraph.replace(\"&lt;/ext&gt;\",\"</mark>\")\n",
    "\n",
    "\n",
    "\n",
    "f.write(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' yards, car-parks, roads, pavements, walls, gates or fences',\n",
       "  '2724$ unless '),\n",
       " (' land, roads, pavements,',\n",
       "  '6799$ piers, jetties, bridges, culverts or excavations; g antiques, works of art, rare books, jewellery, precious stones, articles of gold, silver or other precious metals or furs; h property covered by any marine policy of insurance; i securities and other negotiable instruments, coin, bank and currency notes, cheques, National Giro payment orders, money and postal orders, postage and revenue stamps, National Savings stamps and certificates, holiday pay stamps, trading stamps, luncheon vouchers, credit cards and credit card vouchers;'),\n",
       " ('Money', '11355$Computer Breakdown'),\n",
       " (': This Extension does not cover:- a loss or destruction of or damage to money contained in coin operated gaming or amusement devices or vending machines; b loss or destruction of or damage to Money contained in any vehicle while left unattended c any loss, destruction or damage:- i. arising outside the Territorial Limit',\n",
       "  '11413$ and Section 2 Business Interruption: A. Property Damage: The Company shall not be liable under this Extension in respect of damage to equipment: a in respect of which there is no guarantee, maintenance, rental, hire or lease agreement in force providing a minimum service of on-call remedial or corrective maintenance at inclusive cost, other than damage occasioned by or happening through: i. the negligence of the Insured, or ii. the failure or fluctuation of the supply of electricity to the equipment; b caused by or occasioned through: i. the intentional act or wilful neglect of the Insured; ii. wear and tear, gradual deterioration, gradually developing defects, corrosion, rust or scratching or chipping of painted or polished surface'),\n",
       " ('notwithstanding Exclusion 1.f. arising from dishonesty on the part of any employee of the Insured not discovered within thirty (30) days of the occurrence; iii. occasioned by errors or omissions; iv. resulting from a safe or strong room being opened by a key if the key be left in the Premises out of Business Hours; v. recoverable from any security carrier',\n",
       "  '12164$any testing, repairing, adjusting, servicing or maintenance operation. B. Business Interruption: The Company shall not be liable under this Extension in respect of:- a loss sustained during the first 24 hours following damage to equipment in respect of which there is no guarantee, maintenance, rental, hire or lease agreement in force providing a minimum service of on-call remedial or corrective maintenance at inclusive cost, other than damage occasioned by or happening through the failure or fluctuation of the supply of electricity to the equipment b loss resulting from damage caused by or occasioned through:- i. the intentional act or wilful neglect of the Insured; ii. wear and tear gradual deterioration, gradually developing defects, corrosion, rust, or scratching or chipping of painted or polished surfaces; iii. any testing, repairing, adjusting, servicing or maintenance operation'),\n",
       " ('Personal Accident (Assault)', '13096$Money'),\n",
       " ('shall not operate in respect of persons carrying Money belonging to the Insured:- a outside the Territorial Limits; b who are over seven',\n",
       "  '13157$does not cover:- a loss or destruction of or damage to money contained in coin operated gaming or amusement devices or vending machines; b loss or destruction of or damage to Money contained in any vehicle while left unattended c any loss, destruction or damage:- i. arising outside the Territorial Limits; ii. notwithstanding Exclusion 1.f. arising from dishonesty on the part of any employee of the Insured not discovered within thir'),\n",
       " ('years of age or have not reached their eighteenth (18th) birthday.',\n",
       "  '13600$days of the occurrence; iii. occasioned by errors or omissions; iv. resulting from a safe or strong room being opened by a key if the key be left in the Premises out of Business Hours; v. recoverable from any security carrier. ')]"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Section 1 Property Damage '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# from azure.cosmos import exceptions, CosmosClient, PartitionKey\n",
    "\n",
    "# cosmo_endpoint=\"https://nf-poc-cdb-sql.documents.azure.com\"\n",
    "# cosmo_key=\"iEcEfrxYe0Fm9QtoxDrOpLvGsfzjowwybULlWT9Uz4XxV4RmOIAnRuLdgRFUu1LPU5Vwk3UGivRrPrxnk7083w==\"\n",
    "# client = CosmosClient(cosmo_endpoint, cosmo_key)\n",
    "# database=client.get_database_client('policy-analysis')\n",
    "# tt_documents_container=database.get_container_client('trace_files_dev')\n",
    "# trace_result_container=database.get_container_client('trace_results_dev')\n",
    "\n",
    "# # In[12]:\n",
    "\n",
    "\n",
    "# wording_query=\"SELECT * FROM c WHERE c.file_type ='base wording' \"\n",
    "\n",
    "# wordings = list(tt_documents_container.query_items(\n",
    "#     query=wording_query,\n",
    "#     enable_cross_partition_query=True\n",
    "# ))\n",
    "\n",
    "\n",
    "# # In[19]:\n",
    "\n",
    "\n",
    "# document_query=\"SELECT * FROM c WHERE c.file_type ='policy document'  \"\n",
    "\n",
    "# documents = list(tt_documents_container.query_items(\n",
    "#     query=document_query,\n",
    "#     enable_cross_partition_query=True\n",
    "# ))\n",
    "\n",
    "\n",
    "# # In[29]:\n",
    "# result_query=\"SELECT * FROM c\"\n",
    "\n",
    "\n",
    "\n",
    "# results = list(trace_result_container.query_items(\n",
    "#     query=result_query,\n",
    "#     enable_cross_partition_query=True\n",
    "# ))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_id=[i['id'] for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for wording in wordings:\n",
    "    \n",
    "    refCode_wording=wording[\"refCode\"]\n",
    "\n",
    "    for document in documents:\n",
    "        refCode_document=document[\"refCode\"]\n",
    "        wording_start_date=wording['effectiveDate'] \n",
    "        wording_end_date=wording['expiryDate'] \n",
    "        document_start_date=document['effectiveDate']\n",
    "        if refCode_document==refCode_wording and  wording_start_date<=document_start_date<=wording_end_date:\n",
    "            k+=1\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_set=documents[50:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chubb Multinational PDBI - UK - 04-2018 (SE - from 01-01-2019) - SPECIMENREDACTED_FB1D7BDB-BD05-4275-BE2A-DB30544D967F\n",
      "Already Exist........\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_query=\"SELECT * FROM c\"\n",
    "\n",
    "results = list(trace_result_container.query_items(\n",
    "    query=result_query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "\n",
    "results_id=[i['id'] for i in results]\n",
    "\n",
    "final_list=[]\n",
    "for wording in wordings:\n",
    "    \n",
    "    refCode_wording=wording[\"refCode\"]\n",
    "\n",
    "    for document in documents:\n",
    "        refCode_document=document[\"refCode\"]\n",
    "        wording_start_date=wording['effectiveDate'] \n",
    "        wording_end_date=wording['expiryDate'] \n",
    "        document_start_date=document['effectiveDate']\n",
    "        try:\n",
    "            if refCode_document==refCode_wording:            \n",
    "                wording_content=wording['contents']\n",
    "                document_content=document['contents']\n",
    "                \n",
    "\n",
    "\n",
    "                document_name=list(document_content.keys())[0]\n",
    "\n",
    "                wording_name=list(wording_content.keys())[0]\n",
    "                id_name=wording_name.replace(\".pdf\",\"\")+document_name.replace(\".pdf\",\"\")\n",
    "                print(id_name)\n",
    "                \n",
    "                \n",
    "                if id_name in results_id:\n",
    "                    print('Already Exist........')\n",
    "                    continue\n",
    "\n",
    "                final_list.append([wording_name,document_name,refCode_document,refCode_wording,wording[\"effectiveDate\"],document[\"effectiveDate\"]])\n",
    "\n",
    "                doc_content=document_content[document_name]\n",
    "                word_content=wording_content[wording_name]\n",
    "\n",
    "                print(document_name)\n",
    "                matches={}\n",
    "\n",
    "                total_category_MR=[]\n",
    "                word_content[0].pop(\"pages\", None)\n",
    "                doc_content[0].pop(\"pages\", None)\n",
    "\n",
    "                for category in word_content[0].keys():\n",
    "\n",
    "                    category_content_wording=word_content[0][category]\n",
    "                    category_content_document=doc_content[0][category]\n",
    "                    results=[]\n",
    "                    total_section_MR=[]\n",
    "                    total_section_score=0\n",
    "                    total_sections=0\n",
    "    #                 print(category_content_document.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    for section_w in category_content_wording.keys():\n",
    "                        new_dict ={}\n",
    "                        keyFlag=False\n",
    "                        ##keys are present in both the wording and document\n",
    "\n",
    "                        if section_w not in category_content_document.keys():\n",
    "                            for section_d in category_content_document.keys():\n",
    "\n",
    "                                if section_w in section_d or section_d in section_w:\n",
    "                                    key_wording=section_w\n",
    "                                    key_document=section_d\n",
    "                                    keyFlag=True\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            key_wording=section_w\n",
    "                            key_document=section_w\n",
    "\n",
    "                            keyFlag=True\n",
    "\n",
    "                        if keyFlag:\n",
    "    #                         print(key_wording,\"*\",key_document,category)\n",
    "                            section_content,pages=category_content_document[key_document]\n",
    "\n",
    "                            wording_section_value,wording_section_pages=category_content_wording[key_wording]\n",
    "                            pages.sort()\n",
    "\n",
    "                            wording_section_pages.sort()\n",
    "                            ##when wording pages and document pages are there we will compare\n",
    "                            if len(pages)>=1 and len(wording_section_pages)>=1:\n",
    "\n",
    "                                key_values,dist_,start=get_differences(wording_section_value,section_content,category)\n",
    "                                \n",
    "\n",
    "                                score=SequenceMatcher(None,wording_section_value,section_content).ratio()*100\n",
    "                                if not key_values and not dist_:\n",
    "                                    score=100\n",
    "                                if \"SPECIAL EXTENSIONS APPLICABLE\" in  section_w :\n",
    "                                    score=100\n",
    "                                    \n",
    "\n",
    "                                matchType,relevance=search_relevance(score)\n",
    "                                total_section_score+=score\n",
    "                                total_sections+=1\n",
    "                                new_dict[\"section\"]=key_document\n",
    "\n",
    "\n",
    "                                if (key_values or  dist_) and score <99:\n",
    "\n",
    "\n",
    "                                    content_html= get_html_with_diff(document_name,pages,key_values,dist_,start)\n",
    "\n",
    "\n",
    "                                    new_dict[\"content\"]=content_html\n",
    "\n",
    "                                new_dict[\"relevance\"] = relevance\n",
    "                                new_dict[\"matchType\"] = matchType\n",
    "                                new_dict[\"score\"]=score\n",
    "                                new_dict[\"document_page\"]=pages[0]\n",
    "\n",
    "\n",
    "                                new_dict[\"wording_page\"]=wording_section_pages[0]\n",
    "\n",
    "\n",
    "                                results.append(new_dict)\n",
    "                                total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\"matchType\":new_dict[\"matchType\"]})\n",
    "\n",
    "                            else:\n",
    "                                ##if there are documnet pages\n",
    "                                ##elif there are wording pages\n",
    "                                if len(pages)>=1:\n",
    "                                    new_dict[\"section\"]=key_document\n",
    "                                    new_dict[\"content\"]=\"<ext>{}</ext>\".format(get_html(document_name,pages))\n",
    "\n",
    "                                    new_dict[\"document_page\"]=pages[0]\n",
    "                                    new_dict[\"relevance\"] = \"\"\n",
    "                                    new_dict[\"matchType\"] = \"no\"\n",
    "                                    new_dict[\"score\"]=0\n",
    "\n",
    "                                    results.append(new_dict)\n",
    "                                    total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\"matchType\":new_dict[\"matchType\"]})\n",
    "\n",
    "\n",
    "                                elif len(wording_section_pages)>=1:\n",
    "                                    new_dict[\"section\"]=key_wording\n",
    "                                    new_dict[\"content\"]=\"<dist>{}</dist>\".format(get_html(wording_name,wording_section_pages,doc_path=r\"C:\\Users\\LATEJAS\\Videos\\PDBI\\wordings\"))\n",
    "\n",
    "                                    new_dict[\"wording_page\"]=wording_section_pages[0]\n",
    "                                    new_dict[\"relevance\"] = \"\"\n",
    "                                    new_dict[\"matchType\"] = \"no\"\n",
    "                                    new_dict[\"score\"]=0\n",
    "\n",
    "                                    results.append(new_dict)\n",
    "                                    total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\"matchType\":new_dict[\"matchType\"]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        ###if the key it not there in document we should show wording pages with \"No match\"\n",
    "                        if not keyFlag:\n",
    "                            \n",
    "                            _,pages_word=category_content_wording[section_w]\n",
    "\n",
    "                            if len(pages_word)>=1:\n",
    "                                pages_word.sort()\n",
    "                                new_dict[\"section\"]=section_w\n",
    "\n",
    "\n",
    "                                new_dict[\"content\"]=\"<dist>{}</dist>\".format(get_html(wording_name,pages_word,doc_path=r\"C:\\Users\\LATEJAS\\Videos\\PDBI\\wordings\"))\n",
    "                                new_dict[\"relevance\"] = \"\"\n",
    "                                new_dict[\"matchType\"] = \"no\"\n",
    "                                new_dict[\"score\"]=0\n",
    "                                new_dict[\"wording_page\"]=pages_word[0]\n",
    "                                results.append(new_dict)\n",
    "                                total_section_MR.append({\"relevance\":new_dict[\"relevance\"],\n",
    "                                                                 \"matchType\":new_dict[\"matchType\"]})\n",
    "                    total_section_matchType,total_section_relevance=getMatchTypeRelevance(total_section_MR)\n",
    "                    matches[category]= {\"results\":results,\n",
    "                                        \"matchType\": total_section_matchType,\"relevance\":total_section_relevance}\n",
    "                    total_category_MR.append({\"relevance\":total_section_relevance,\n",
    "                                                         \"matchType\":total_section_matchType})  \n",
    "\n",
    "                total_category_matchType,total_category_relevance=getMatchTypeRelevance(total_category_MR)\n",
    "                \n",
    "                insert_item={\n",
    "                              \"policyDetails\": {\n",
    "                                    \"policyNo\": document[\"policyNo\"],\n",
    "                                    \"expiryDate\": document[ \"expiryDate\"],\n",
    "                                    \"effectiveDate\": document[\"effectiveDate\"]\n",
    "                                  },\n",
    "                            \"title\": document_name.replace(\".pdf\",\"\"),\n",
    "                            \"id\":id_name,\n",
    "                            \"wordingTitle\":wording_name.replace(\".pdf\",\"\"),\n",
    "                            \"matchType\": total_category_matchType,\n",
    "\n",
    "                            \"relevance\":total_category_relevance,\n",
    "\n",
    "                            \"refCode\": document[\"refCode\"],\n",
    "\n",
    "                             \"country\":\"GB\",\n",
    "                            \"lob\":document[\"lob\"],\n",
    "                            \"matches\": matches\n",
    "                            }\n",
    "                f=open(\"bb\"+document['title']+\".json\",\"w\")\n",
    "\n",
    "                f.write(json.dumps(insert_item))\n",
    "                print(\"*************************************************\")  \n",
    "                try:\n",
    "                    trace_result_container.create_item(body=insert_item)\n",
    "                except Exception as e:\n",
    "                    print(\"exception block\",wording_name,document_name,e)\n",
    "                    try:\n",
    "                        trace_result_container.replace_item(item=document_name, body=insert_item)\n",
    "                    except Exception as e1:\n",
    "                        print(\"exception block1\",e1,wording_name,document_name,e1)\n",
    "        except Exception as err:\n",
    "            print(\"higher level exception\",err,wording_name,document_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_keys={}\n",
    "# for key_w in word_exclusion.keys():\n",
    "#     keyFlag=False\n",
    "#     key_wording=key_w\n",
    "#     if key_w not in doc_exclusion.keys():\n",
    "#         for key_doc in doc_exclusion.keys():\n",
    "\n",
    "#             if key_w in key_doc or key_doc in key_w:\n",
    "#                 key_wording=key_w\n",
    "#                 key_document=key_doc\n",
    "#                 keyFlag=True\n",
    "#                 print(key_wording,\"*\",key_document)\n",
    "#     else:\n",
    "#         key_wording=key_w\n",
    "#         key_document=key_w\n",
    "#         print(key_wording,\"*\",key_document)\n",
    "#         keyFlag=True\n",
    "        \n",
    "#     if not keyFlag:\n",
    "#         print(key_w)\n",
    "        \n",
    "    \n",
    "    \n",
    "                    \n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REDACTED_FB1D7BDB-BD05-4275-BE2A-DB30544D967F.pdf'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ',\n",
       "  '1959$b) overnight such vehicle be housed in a securely locked building or compound;'),\n",
       " (' ',\n",
       "  '2440$ i subsidence, ground heave or landslip: i. to any Building (or Contents therein) arising from the settlement or movement of made-up ground or of ground over mine workings or on any site where there has been a previous occurrence of subsidence, ground heave or landslip; ii. to land, yards, car-parks, roads, pavements, walls, gates or fences unless a Building insured hereby is also affected; iii. resulting from coastal or river erosion or from any building, demolition or excavation works at the Premises unless resulting from fire, explosion, Earthquake or the escape of water from any tank, apparatus or pipe, or iv. commencing prior to the granting of this insurance; j its own collapse or the normal settling, cracking, shrinkage or expansion of the building or the site or any parts thereof on which the property described within stands; k the undernoted perils occurring while any building is vacant, disused or ceases to be operated:- i. theft ii. malicious act or vandalism except destruction or damage caused by fire or explosion iii. escape of water or oil from any water or heating installation; l solidification of the contents of molten material holding units, molten material transmission lines and/or appurtenances (unless such loss, destruction or damage is directly caused by any peril not otherwise excluded by this Policy); m the escape of molten material from any furnace, mould, ladle, holding unit, transmission line or appurtenance (but excepting that ensuing from destruction or damage caused by fire or explosion to the extent not otherwise excluded by this Policy); n Storm or Flood to fences, gates, drains, sewers or water courses, nor the cost of cleaning them or making them good; o asbestos material removal unless the asbestos is itself damaged by fire, lightning, explosion, aircraft or other aerial devices or articles dropped therefrom, riot, civil commotion, strikers, locked out workers, or persons taking part in labour disturbances or malicious persons acting on behalf of or in connection with any political organisation, Earthquake, impact by any road vehicle or animal, Storm, Flood or escape of water from any tank or apparatus; or demolition or increased cost of reconstruction, repair, debris removal or loss of use necessitated by the enforcement of any law or ordinance regulating asbestos material; or any governmental direction or request declaring that asbestos material present in or part of or utilised on any undamaged portion of the Property Insured can no longer be used for the purpose for which it was intended or installed and must be removed or modified.  2. the bursting by steam pressure of a boiler, economiser, vessel, machine or apparatus in which internal pressure is due to steam only and belonging to or under the control of the Insured other than in respect of:- a Section 1 - a boiler used for domestic purposes only;  b Section 2 - any boiler or economiser on the Premises or a boiler used for domestic purposes only; c Sections 1 and 2 - resulting damage to surrounding Property Insured by this Policy.  3. loss, destruction or damage caused by or occasioned through the cracking, fracturing, collapse or overheating of boilers, economisers, vessels, tubes or pipes, nipple leakage and/or the failure of welds of boilers nor any consequential loss of whatsoever nature resulting or arising therefrom.'),\n",
       " ('  ',\n",
       "  '5814$ 4. any consequential loss of whatsoever nature resulting or arising from the deliberate act of any supply authority in withholding the supply of water, gas, electricity or fuel or telecommunications services - but this shall not exclude:- a such loss if resulting from a cause which is not otherwise excluded, or '),\n",
       " (' ',\n",
       "  '6211$ 5. loss, destruction or damage attributable solely to change in the water table level.  6. loss or destruction of or damage to:- a aircraft or aerospatial devices, watercraft, motor vehicles licensed for road use, railway locomotives or rolling stock or their accessories; b property in transit whether by air, sea, road, rail or inland waterway; c that portion of any electrical apparatus, appliance or device of any kind (including wiring) caused by its own over-running, excessive pressure, short-circuiting, self-heating or leakage of electricity; d livestock, growing crops or standing timber;  e explosives; f land, roads, pavements, piers, jetties, bridges, culverts or excavations; g antiques, works of art, rare books, jewellery, precious stones, articles of gold, silver or other precious metals or furs; h property covered by any marine policy of insurance; i securities and other negotiable instruments, coin, bank and currency notes, cheques, National Giro payment orders, money and postal orders, postage and revenue stamps, National Savings stamps and certificates, holiday pay stamps, trading stamps, luncheon vouchers, credit cards and credit card vouchers; j that part of the Property Insured which is actually being worked upon or undergoing any process, directly resulting therefrom or caused by any testing, repairing, adjusting, servicing or maintenance operation; k movable property in the open by the action of wind, rain, hail, snow, sleet, frost, Flood or dust; l property or structures in the course of construction or erection and materials or supplies in connection with all such property in the course of construction or erection; m water other than water which is normally contained within any tank, piping system or other process equipment; n overhead electrical and telecommunication transmission and distribution lines, overhead transformers or other similar overhead communication, transmission or distribution equipment and their supporting structures'),\n",
       " (' ',\n",
       "  '9983$ However, in the event that a peril listed below results from any of the matters described above, this Policy, subject to all its terms, conditions and exclusions, will cover physical damage occurring during the Period of Insurance to Property Insured by this Policy directly caused by such listed peril.                                                                                                   Listed Perils - Fire, Explosion '),\n",
       " ('or damage ', '11348$resulting from Breakdown of equipment '),\n",
       " ('for the amount of loss sustained during the first twenty four (24) hours following the Breakdown',\n",
       "  '11386$unless interruption of or interference with the Business exceeds twenty four (24) hours'),\n",
       " ('thirty (30', '12001$fifteen (15'),\n",
       " ('eigh', '12541$fif')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={\"Conditions\":\"abc\"}\n",
    "b={\"special Conditions\":\"def\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "partial_ratio = fuzz.partial_ratio( \"special Conditions\",\" Section 4 Conditions\")\n",
    "print(partial_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                         if total_sections!=0:\n",
    "#                             total_section_score= round(total_section_score/total_sections,2)\n",
    "\n",
    "#                         total_section_matchType=\"Full\"\n",
    "#                         if total_section_score<99.5:\n",
    "#                             total_section_matchType=\"partial\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
